<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://metal3.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://metal3.io/" rel="alternate" type="text/html" /><updated>2020-07-26T14:26:51+00:00</updated><id>https://metal3.io/feed.xml</id><title type="html">Metal³ - Metal Kubed</title><subtitle>Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.</subtitle><entry><title type="html">Introducing the Metal3 IP Address Manager</title><link href="https://metal3.io/blog/2020/07/06/IP_address_manager.html" rel="alternate" type="text/html" title="Introducing the Metal3 IP Address Manager" /><published>2020-07-06T00:00:00+00:00</published><updated>2020-07-06T00:00:00+00:00</updated><id>https://metal3.io/blog/2020/07/06/IP_address_manager</id><content type="html" xml:base="https://metal3.io/blog/2020/07/06/IP_address_manager.html">&lt;p&gt;As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4
release, the Metal3 crew introduced a new project : its own IP Address Manager.
This blog post will go through the motivations behind such a project, the
features that it brings, its use in Metal3 and the future work.&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-ip-address-manager-&quot;&gt;What is the IP Address Manager ?&lt;/h2&gt;

&lt;p&gt;The IP Address Manager (IPAM) is a controller that provides IP addresses and
manages the allocations of IP subnets. It is not a DHCP server in that it only
reconciles Kubernetes objects and does not answer to any DHCP queries. It
allocates IP addresses on request, but does not handle any use of those
addresses.&lt;/p&gt;

&lt;p&gt;This sounds like the description of any IPAM system, no ? Well the twist
is that this manager is based on Kubernetes to specifically handle some
constraints from Metal3. We will go through the different issues that this
project tackles.&lt;/p&gt;

&lt;p&gt;When deploying nodes in bare metal environment, there are a lot of possible
variations. This project specifically aims to solve the cases where static
IP address configurations are needed. It is designed to specifically address
this in the &lt;a href=&quot;https://cluster-api.sigs.k8s.io/&quot;&gt;Cluster API (CAPI) context&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;CAPI addresses the deployment of Kubernetes clusters and nodes, using
the Kubernetes API. As such, it uses objects such as Machine Deployments
(similar to deployments for pods) that takes care of creating the requested
number of machines, based on templates. The replicas can be increased by the
user, triggering the creation of new machines based on the provided templates.
This mechanism does not allow for flexibility to be able to provide static
addresses for each machine. The manager adds this flexibility by providing
the address right before provisioning the node.&lt;/p&gt;

&lt;p&gt;In addition, all the resources from the source cluster must support the CAPI
pivoting, i.e. being copied and recreated in the target cluster. This means
that all objects must contain all needed information in their spec field to
recreate the status in the target cluster without losing information. All
objects must, through a tree of owner references, be attached to the cluster
object, for the pivoting to proceed properly.&lt;/p&gt;

&lt;p&gt;In a nutshell, the manager provides an IP Address allocation service, based
on Kubernetes API and fulfilling the needs of Metal3, specifically the
requirements of CAPI.&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work-&quot;&gt;How does it work ?&lt;/h2&gt;

&lt;p&gt;The manager follows the same logic as the volume allocation in Kubernetes,
with a claim and an object created for that claim. There are three type of
objects defined, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt;, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; and the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt; objects.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; objects contain the definition of the IP subnets from which the
Addresses are allocated. It supports both IPv4 and IPv6. The subnets can either
be defined as such or given as a start and end IP addresses with a prefix.
It also supports pre-allocating IP addresses.&lt;/p&gt;

&lt;p&gt;The following is an example &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; definition :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipam.metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IPPool&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pools&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.10&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.30&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.1&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;subnet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.1/26&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;subnet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.128/25&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;24&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;preAllocations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;claim2&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;An IPv6 &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; would be defined similarly :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipam.metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IPPool&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pools&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2001:0db8:85a3:0000:0000:8a2e::10&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2001:0db8:85a3:0000:0000:8a2e:ffff:fff0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;96&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;12001:0db8:85a3:0000:0000:8a2e::1&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;subnet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2001:0db8:85a3:0000:0000:8a2d::/96&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;96&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2001:0db8:85a3:0000:0000:8a2d::1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Whenever something requires an IP address from the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt;, it will create an
&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt;. The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; contains a pointer to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; and an owner reference
to the object that created it.&lt;/p&gt;

&lt;p&gt;The following is an example of an &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipam.metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IPClaim&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;claim1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1-192-168-0-13&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The controller will then reconcile this object and allocate an IP address. It
will create an &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt; object representing the allocated address. It will
then update the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; status to list the IP Address and the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; status
to point to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following is an example of an &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipam.metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IPAddress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1-192-168-0-13&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;claim&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;claim1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.13&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;24&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this allocation, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; will be looking like :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipam.metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IPPool&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pool1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pools&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.10&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.30&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;25&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.1&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;subnet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.1/26&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;subnet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.128/25&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;24&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.1.1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;preAllocations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;claim2&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.12&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;claim1&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.13&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;claim2&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;use-in-metal3&quot;&gt;Use in Metal3&lt;/h2&gt;

&lt;p&gt;The IP Address Manager is used in Metal3 together with the metadata and network
data templates feature. Each Metal3Machine (M3M) and Metal3MachineTemplate
(M3MT) is associated with a Metal3DataTemplate that contains a metadata and /
or a network data template that will be rendered for each Metal3Machine. The
rendered data will then be provided to Ironic. Those templates reference
&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt; objects. For each Metal3Machine, an &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; is created for each
&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPPool&lt;/code&gt;, and the templates are rendered with the allocated &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is how we achieve dynamic IP Address allocations in setups that
require static configuration, allowing us to use Machine Deployment and Kubeadm
Control Plane objects from CAPI in hardware labs where DHCP is not supported.&lt;/p&gt;

&lt;p&gt;Since each &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt; has an owner reference set to its &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; object, and
&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; objects have an owner reference set to the Metal3Data object created
from the Metal3DataTemplate, the owner reference chain links a Metal3Machine to
all the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPClaim&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IPAddress&lt;/code&gt; objects created for it, allowing for CAPI
pivoting.&lt;/p&gt;

&lt;h2 id=&quot;what-now-&quot;&gt;What now ?&lt;/h2&gt;

&lt;p&gt;The project is fulfilling its basic requirements, but we are looking into
extending it and covering more use-cases. For example we are looking at
adding an integration with Infoblox and other external IPAM services. Do not
hesitate to open an issue if you have some ideas for new features!&lt;/p&gt;

&lt;p&gt;The project can be found
&lt;a href=&quot;https://github.com/metal3-io/ip-address-manager&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Maël Kimmerlin</name></author><summary type="html">As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4 release, the Metal3 crew introduced a new project : its own IP Address Manager. This blog post will go through the motivations behind such a project, the features that it brings, its use in Metal3 and the future work.</summary></entry><entry><title type="html">Raw image streaming available in Metal3</title><link href="https://metal3.io/blog/2020/07/05/raw-image-streaming.html" rel="alternate" type="text/html" title="Raw image streaming available in Metal3" /><published>2020-07-05T00:00:00+00:00</published><updated>2020-07-05T00:00:00+00:00</updated><id>https://metal3.io/blog/2020/07/05/raw-image-streaming</id><content type="html" xml:base="https://metal3.io/blog/2020/07/05/raw-image-streaming.html">&lt;p&gt;Metal3 supports multiple types of images for deployment, the most
popular being QCOW2. We have recently added support for a feature of Ironic
that improves deployments on constrained environments, raw image streaming.
We’ll first dive into how Ironic deploys the images on the target hosts, and
how raw image streaming improves this process. Afterwards, we will point out
the changes to take this into use in Metal3.&lt;/p&gt;

&lt;h2 id=&quot;image-deployments-with-ironic&quot;&gt;Image deployments with Ironic&lt;/h2&gt;

&lt;p&gt;In Metal3, the image deployment is performed by the Ironic Python Agent (IPA)
image running on the target host. In order to deploy an image, Ironic will
first boot the target node with an IPA image over iPXE. IPA will run in memory.&lt;/p&gt;

&lt;p&gt;Once IPA runs on the target node, Ironic will instruct it to download the
target image. In Metal3, we use HTTP(S) for the download of the image. IPA will
download the image and, depending on the format of the image, prepare it to
write it on the disk. This means that the image is downloaded in memory and
decompressed, two steps that can be both time and memory consuming.&lt;/p&gt;

&lt;p&gt;In order to improve this process, Ironic implemented a feature called raw image
streaming.&lt;/p&gt;

&lt;h2 id=&quot;what-is-raw-image-streaming-&quot;&gt;What is raw image streaming ?&lt;/h2&gt;

&lt;p&gt;The target image format when writing to disk is raw. That’s why the images in
formats like QCOW2 must be processed before being written to disk. However, if
the image that is downloaded is already in raw format, then no processing is
needed.&lt;/p&gt;

&lt;p&gt;Ironic leverages this, and instead of first downloading the image and then
processing it before writing it to disk, it will directly write the
downloading image to the disk. This feature is known as image streaming.
Image streaming can only be performed with images in raw format.&lt;/p&gt;

&lt;p&gt;Since the downloaded image when streamed is directly written to disk, the
memory size requirements changes. For any other format than raw, the target
host needs to have sufficient memory to both run IPA (4GB) and
download the image in memory. However with raw images, the only constraint
on memory is to run IPA (so 4GB). For example, in order to deploy an Ubuntu
image (around 700MB, QCOW2), the requirement is 8GB when in QCOW2 format, while
it is only 4GB (as for any other image) when streamed as raw. This allows to
deploy images that are bigger than the available memory on constrained nodes.&lt;/p&gt;

&lt;p&gt;However, this shifts the load on the network, since the raw images are usually
much bigger than other formats. Using this feature in network constrained
environment is not recommended.&lt;/p&gt;

&lt;h2 id=&quot;raw-image-streaming-in-metal3&quot;&gt;Raw image streaming in Metal3&lt;/h2&gt;

&lt;p&gt;In order to use raw image streaming in Metal3, a couple of steps are needed.
The first one is to convert the image to raw and make it available in an
HTTP server. This can be achieved by running :&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;qemu-img convert &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; raw &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;IMAGE_NAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;IMAGE_RAW_NAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once converted the image format needs to be provided to Ironic through the
BareMetalHost (BMH) image spec field. If not provided, Ironic will assume that
the format is unspecified and download it in memory first.&lt;/p&gt;

&lt;p&gt;The following is an example of the BMH image spec field in Metal3 Dev Env.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BareMetalHost&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;checksum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;checksumType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;md5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If deploying with Cluster API provider Metal3 (CAPM3), CAPM3 takes care of
setting the image field of BMH properly, based on the image field values in
the Metal3Machine (M3M), that might be based on a Metal3MachineTemplate (M3MT).
So in order to use raw image streaming, the format of the image must be
provided in the image spec field of the Metal3Machine or Metal3MachineTemplate.&lt;/p&gt;

&lt;p&gt;The following is an example of the M3M image spec field in metal3-dev-env :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;infrastructure.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Metal3Machine&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;checksum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;checksumType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;md5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following is for a M3MT in metal3-dev-env :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;infrastructure.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Metal3MachineTemplate&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;checksum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;checksumType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;md5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will enable raw image streaming. By default, metal3-dev-env uses raw image
streaming, in order to minimize the resource requirements of the environment.&lt;/p&gt;

&lt;h2 id=&quot;in-a-nutshell&quot;&gt;In a nutshell&lt;/h2&gt;

&lt;p&gt;With the addition of raw image streaming, Metal3 now supports a wider range of
hardware, specifically the memory constrained nodes and speeds up deployments.
Metal3 still supports all the other formats it supported until now. This new
feature changes the way raw images are deployed for better efficiency.&lt;/p&gt;</content><author><name>Maël Kimmerlin</name></author><summary type="html">Metal3 supports multiple types of images for deployment, the most popular being QCOW2. We have recently added support for a feature of Ironic that improves deployments on constrained environments, raw image streaming. We’ll first dive into how Ironic deploys the images on the target hosts, and how raw image streaming improves this process. Afterwards, we will point out the changes to take this into use in Metal3.</summary></entry><entry><title type="html">Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster</title><link href="https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html" rel="alternate" type="text/html" title="Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster" /><published>2020-06-18T00:00:00+00:00</published><updated>2020-06-18T00:00:00+00:00</updated><id>https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment</id><content type="html" xml:base="https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This blog post describes how to deploy a bare metal cluster, a virtual one for simplicity, using &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env&quot;&gt;Metal³/metal3-dev-env&lt;/a&gt;. We will briefly discuss the steps involved in setting up the cluster as well as some of the customization available. If you want to know more about the architecture of Metal³, this &lt;a href=&quot;/blog/2020/02/27/talk-kubernetes-finland-metal3.html&quot;&gt;blogpost&lt;/a&gt; can be helpful.&lt;/p&gt;

&lt;p&gt;This post builds upon the &lt;a href=&quot;/blog/2020/02/18/metal3-dev-env-install-deep-dive.html&quot;&gt;detailed metal3-dev-env walkthrough blogpost&lt;/a&gt; which describes in detail the steps involved in the environment set up and management cluster configuration. Here we will use that environment to deploy a new Kubernetes cluster using Metal³.&lt;/p&gt;

&lt;p&gt;Before we get started, there are a couple of requirements we are expecting to be fulfilled.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Metal³ is already deployed and working,  if not please follow the instructions in the previously mentioned &lt;a href=&quot;/blog/2020/02/18/metal3-dev-env-install-deep-dive.html&quot;&gt;detailed metal3-dev-env walkthrough blogpost&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The appropriate environment variables are setup via shell or in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;config_${user}.sh&lt;/code&gt; file, for example -
    &lt;ul&gt;
      &lt;li&gt;CAPI_VERSION&lt;/li&gt;
      &lt;li&gt;NUM_NODES&lt;/li&gt;
      &lt;li&gt;CLUSTER_NAME&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview-of-config-and-resource-types&quot;&gt;Overview of Config and Resource types&lt;/h2&gt;
&lt;p&gt;In this section we give a brief overview of the important config files and resources used as part of the bare metal cluster deployment. 
The following sub-sections show the config files and resources that are created and give a brief description about some of them. This will help you understand the technical details of the cluster deployment. You can also choose to skip this section, visit the next section about &lt;em&gt;provisioning&lt;/em&gt; first and then revisit this.&lt;/p&gt;

&lt;h3 id=&quot;config-files-and-resources-types&quot;&gt;Config Files and Resources Types&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-06-18-Metal3-dev-env-BareMetal-Cluster-Deployment/manifest-directory.png&quot; alt=&quot;&amp;quot;The directory tree for the ansible role used for deployment&amp;quot;&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;Among these the config files are rendered under the path &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/files&lt;/code&gt; as part of the provisioning process.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A description of some of the files part of provisioning a cluster, in a centos based environment :&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Path&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(de)provisioning scripts&lt;/td&gt;
      &lt;td&gt;Scripts to trigger provisioning or deprovisioning of cluster, control plane or worker&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/scripts/v1alphaX/&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/templates&quot;&gt;templates directory&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Templates for cluster, control plane, worker definitions&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/templates&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clusterctl env file&lt;/td&gt;
      &lt;td&gt;Cluster parameters and details&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${Manifests}/clusterctl_env_centos.rc&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/tasks/generate_templates.yml&quot;&gt;generate templates&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Renders cluster, control plane and worker definitions in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Manifest&lt;/code&gt; directory&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/tasks/generate_templates.yml&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/vars/main.yml&quot;&gt;main vars file&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Variable file that assigns all the defaults used during deployment&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/vars/main.yml&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Here are some of the resources that are created as part of provisioning :&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Cluster&lt;/td&gt;
      &lt;td&gt;a Cluster API resource for managing a cluster&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Metal3Cluster&lt;/td&gt;
      &lt;td&gt;Corresponding Metal3 resource generated as part of bare metal cluster deployment, and managed by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cluster&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KubeadmControlPlane&lt;/td&gt;
      &lt;td&gt;Cluster API resource for managing the control plane, it also manages the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt; object, and has the &lt;strong&gt;KubeadmConfig&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MachineDeployment&lt;/td&gt;
      &lt;td&gt;Cluster API resource for managing workers via &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;MachineSet&lt;/code&gt; object, it can be used to add/remove workers by scaling Up/Down&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MachineSet&lt;/td&gt;
      &lt;td&gt;Cluster API resource for managing &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt; objects for worker nodes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Machine&lt;/td&gt;
      &lt;td&gt;Cluster API resource for managing nodes - control plane or workers. In case of Controlplane, its directly managed by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KubeadmControlPlane&lt;/code&gt;, whereas for Workers it’s managed by a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;MachineSet&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Metal3Machine&lt;/td&gt;
      &lt;td&gt;Corresponding Metal3 resource for managing bare metal nodes, it’s managed by a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt; resource&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Metal3MachineTemplate&lt;/td&gt;
      &lt;td&gt;Metal3 resource which acts as a template when creating a control plane or a worker node&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KubeadmConfigTemplate&lt;/td&gt;
      &lt;td&gt;A template of &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KubeadmConfig&lt;/code&gt;, for Workers, used to generate KubeadmConfig when a new worker node is provisioned&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : The corresponding &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KubeadmConfig&lt;/code&gt; is copied to the control plane/worker at the time of provisioning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bare-metal-cluster-deployment&quot;&gt;Bare Metal Cluster Deployment&lt;/h2&gt;

&lt;p&gt;The deployment scripts primarily use ansible and the existing Kubernetes management cluster (based on minikube ) for deploying the bare-metal cluster. Make sure that some of the environment variables used for Metal³ deployment are set, if you didn’t use &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;config_${user}.sh&lt;/code&gt; for setting the environment variables.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Default&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPI_VERSION&lt;/td&gt;
      &lt;td&gt;Version of Metal3 API4&lt;/td&gt;
      &lt;td&gt;v1alpha3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;POD_CIDR&lt;/td&gt;
      &lt;td&gt;Pod Network CIDR&lt;/td&gt;
      &lt;td&gt;192.168.0.0/18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLUSTER_NAME&lt;/td&gt;
      &lt;td&gt;Name of bare metal cluster&lt;/td&gt;
      &lt;td&gt;test1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;steps-involved&quot;&gt;Steps Involved&lt;/h3&gt;

&lt;p&gt;All the scripts for cluster provisioning or deprovisioning are located at - &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/scripts/v1alphaX&quot;&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/scripts/v1alphaX/&lt;/code&gt;&lt;/a&gt;. The scripts call a common playbook which handles all the tasks that are available.&lt;/p&gt;

&lt;p&gt;The steps involved in the process are :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The script calls an ansible playbook with necessary parameter ( from env variables and defaults )&lt;/li&gt;
  &lt;li&gt;The playbook executes the role -, &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test&quot;&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/vm-setup/roles/v1aX_integration_test&lt;/code&gt;&lt;/a&gt;, which runs the main &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/tasks/main.yml&quot;&gt;task_file&lt;/a&gt; for provisioning/deprovisioning the cluster, control plane or a worker&lt;/li&gt;
  &lt;li&gt;There are &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/templates&quot;&gt;templates&lt;/a&gt; in the role, which are used to render configurations in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Manifest&lt;/code&gt; directory. These configurations use kubeadm and are supplied to the Kubernetes module of ansible to create the cluster.&lt;/li&gt;
  &lt;li&gt;During provisioning, first the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;clusterctl&lt;/code&gt; env file is generated, then the cluster, control plane and worker definition templates for &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;clusterctl&lt;/code&gt; are generated at &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${HOME}/.cluster-api/overrides/infrastructure-metal3/${CAPM3RELEASE}&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Using the templates generated in previous step, the definitions for resources related to cluster, control plane and worker are rendered using &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;clusterctl&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Centos or Ubuntu image &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/v1aX_integration_test/tasks/download_image.yml&quot;&gt;is downloaded&lt;/a&gt; in the next step.&lt;/li&gt;
  &lt;li&gt;Finally using the above definitions, which are passed to the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;K8s&lt;/code&gt; module in ansible, the corresponding resource( cluster/control plane/worker ) is provisioned.&lt;/li&gt;
  &lt;li&gt;These same definitions are reused at the time of deprovisioning the corresponding resource, again using the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;K8s&lt;/code&gt; module in ansible
    &lt;blockquote&gt;
      &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : The manifest directory is created when provisioning is triggered for the first time and is subsequently used to store the config files that are rendered for deploying the bare metal cluster.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-06-18-Metal3-dev-env-BareMetal-Cluster-Deployment/metal3-bmetal-arch-overview.png&quot; alt=&quot;&amp;quot;An Overview of various resources generated while provisioning and their relationship amongst themselves&amp;quot;&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;provision-cluster&quot;&gt;Provision Cluster&lt;/h3&gt;
&lt;p&gt;This script, located at the path - &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/scripts/v1alphaX/provision_clusters.sh&lt;/code&gt;, provisions the cluster by creating a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal3Cluster&lt;/code&gt; and a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cluster&lt;/code&gt; resource.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
To see if you have a successful Cluster resource creation( the cluster still doesn’t have a control plane or workers ), just do :&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl get Metal3Cluster $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;This will return the cluster deployed, and you can check the cluster details by describing the returned resource.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
Here is what a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cluster&lt;/code&gt; resource looks like :&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl describe Cluster $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Cluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;......&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterNetwork&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pods&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;cidrBlocks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.0.0/18&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;cidrBlocks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.96.0.0/12&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;controlPlaneEndpoint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;192.168.111.249&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6443&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;controlPlaneRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;controlplane.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KubeadmControlPlane&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metal3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;infrastructureRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;infrastructure.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Metal3Cluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metal3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;infrastructureReady&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;phase&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Provisioned&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;provision-controlplane&quot;&gt;Provision Controlplane&lt;/h3&gt;

&lt;p&gt;This script, located at the path - &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env}/scripts/v1alphaX/provision_clusters.sh&lt;/code&gt;, provisions the control plane member of the cluster using the rendered definition of control plane explained in the &lt;strong&gt;Steps Involved&lt;/strong&gt; section. The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KubeadmControlPlane&lt;/code&gt; creates a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt; which picks up a BareMetalHost satisfying its requirements as the control plane node, and it is then provisioned by the Bare Metal Operator. A &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal3MachineTemplate&lt;/code&gt; resource is also created as part of the provisioning process.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;It takes some time for the provisioning of the control plane, you can watch the process using some steps shared a bit later&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl get KubeadmControlPlane $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;span class=&quot;gp&quot;&gt;kubectl describe KubeadmControlPlane $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;controlplane.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KubeadmControlPlane&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;....&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ownerReferences&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/v1alpha3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;blockOwnerDeletion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;controller&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Cluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;aec0f73b-a068-4992-840d-6330bf943d22&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resourceVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;44555&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selfLink&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/apis/controlplane.cluster.x-k8s.io/v1alpha3/namespaces/metal3/kubeadmcontrolplanes/bmetalcluster&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;99487c75-30f1-4765-b895-0b83b0e5402b&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;infrastructureTemplate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;infrastructure.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Metal3MachineTemplate&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster-controlplane&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metal3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kubeadmConfigSpec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;[....]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1.18.0&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/cluster-name=bmetalcluster,cluster.x-k8s.io/control-plane=&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;unavailableReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;updatedReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl get Metal3MachineTemplate $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-controlplane&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To track the progress of provisioning, you can try the following:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get BareMetalHosts -n metal3 -w
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHosts&lt;/code&gt; resource is created when &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal³/metal3-dev-env&lt;/code&gt; was deployed. It is a kubernetes resource that represents a bare metal Machine, with all its details and configuration, and is managed by the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Bare Metal Operator&lt;/code&gt;. You can also use the short representation instead, i.e. &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;bmh&lt;/code&gt; ( short for &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHosts&lt;/code&gt;) in the command above.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;You should see all the nodes that were created at the time of metal3 deployment, along with their current status as the provisioning progresses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;All the bare metal hosts listed above were created when Metal³ was deployed in the &lt;em&gt;detailed metal3-dev-env walkthrough blogpost&lt;/em&gt;.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get Machine -n metal3 -w
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;This shows the status of Machine associated with control plane and we can watch the status of provisioning under PHASE&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
Once the provisioning is finished, let’s get the host-ip :&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;sudo virsh net-dhcp-leases baremetal
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;baremetal&lt;/code&gt; is one of the 2 networks that were created at the time of Metal3 deployment, the other being “provisioning” which is used - as you have guessed - for provisioning the bare metal cluster. More details about networking setup in the metal3-dev-env environment are described in the - &lt;a href=&quot;/blog/2020/02/18/metal3-dev-env-install-deep-dive.html&quot;&gt;detailed metal3-dev-env walkthrough blogpost&lt;/a&gt;.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
You can login to the control plane node if you want, and can check the deployment status using two methods.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ssh metal3@{control-plane-node-ip}
ssh metal3@192.168.111.249
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;provision-workers&quot;&gt;Provision Workers&lt;/h3&gt;

&lt;p&gt;The script is located at &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;${metal3-dev-env-path}/scripts/v1alphaX/provision_worker.sh&lt;/code&gt; and it provisions a node to be added as a worker to the bare metal cluster. It selects one of the remaining nodes and provisions it and adds it to the bare metal cluster ( which only has a control plane node at this point ). The resources created for workers are - &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;MachineDeployment&lt;/code&gt; which can be scaled up to add more workers to the cluster and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;MachineSet&lt;/code&gt; which then creates a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt; managing the node.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;Similar to a control plane provisioning, a worker provisioning also takes some time, and you can watch the process using steps shared a bit later. This will also apply when you scale Up/Down workers at a later point in time.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
This is what a &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;MachineDeployment&lt;/code&gt; looks like&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl describe MachineDeployment $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/v1alpha3&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;MachineDeployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;....&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ownerReferences&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/v1alpha3&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Cluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;aec0f73b-a068-4992-840d-6330bf943d22&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resourceVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;66257&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selfLink&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/apis/cluster.x-k8s.io/v1alpha3/namespaces/metal3/machinedeployments/bmetalcluster&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;uid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;f598da43-0afe-44e4-b793-cd5244c13f4e&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;minReadySeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;progressDeadlineSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;600&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;revisionHistoryLimit&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/cluster-name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;nodepool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nodepool-0&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;rollingUpdate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxSurge&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxUnavailable&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RollingUpdate&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/cluster-name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;nodepool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nodepool-0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;bootstrap&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;configRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bootstrap.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KubeadmConfigTemplate&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster-workers&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;clusterName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;infrastructureRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;infrastructure.cluster.x-k8s.io/v1alpha3&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Metal3MachineTemplate&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bmetalcluster-workers&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1.18.0&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;observedGeneration&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;phase&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ScalingUp&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster.x-k8s.io/cluster-name=bmetalcluster,nodepool=nodepool-0&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;unavailableReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;updatedReplicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
To check the status we can follow steps similar to Controlplane case :&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get bmh -n metal3 -w
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;We can see the live status of the node being provisioned. As mentioned before &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;bmh&lt;/code&gt; is short representation for &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHosts&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;kubectl get Machine -n metal3 -w
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;This shows the status of Machines associated with workers, apart from the one for Controlplane, and we can watch the status of provisioning under PHASE&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;sudo virsh net-dhcp-leases baremetal
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;To get the nodes IP&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ssh metal3@{control-plane-node-ip}
kubectl get nodes
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;To check if its added to the cluster&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;ssh metal3@{node-ip}
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;If you want to login to the node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;kubectl scale --replicas=3 MachineDeployment $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;CLUSTER_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;We can add or remove workers to the cluster, we can scale up the MachineDeployment up or down, in this example we are adding 2 more worker nodes, making the total nodes = 3&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deprovisioning&quot;&gt;Deprovisioning&lt;/h3&gt;

&lt;p&gt;All of the previous components have corresponding deprovisioning scripts which use config files, in the previously mentioned manifest directory, and use them to clean up worker, control plane and cluster.&lt;/p&gt;

&lt;p&gt;This step will use the already generated cluster/control plane/worker definition file, and supply it to &lt;strong&gt;Kubernetes&lt;/strong&gt; ansible module to remove/deprovision the resource. You can find it, under the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Manifest&lt;/code&gt; directory, in the Snapshot shared at the beginning of this blogpost where we show the file structure.&lt;/p&gt;

&lt;p&gt;For example if you wish to deprovision the cluster, you would do :&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;sh $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;metal3-dev-env-path&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/scripts/v1alphaX/deprovision_worker.sh
&lt;span class=&quot;gp&quot;&gt;sh $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;metal3-dev-env-path&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/scripts/v1alphaX/deprovision_controlplane.sh
&lt;span class=&quot;gp&quot;&gt;sh $&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;metal3-dev-env-path&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/scripts/v1alphaX/deprovision_cluster.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note :
The reason for running the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;deprovision_worker.sh&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;deprovision_controlplane.sh&lt;/code&gt; scripts is that not all objects are cleared when we just run the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;deprovision_cluster.sh&lt;/code&gt; script. Following this, if you want to deprovision control plane it is recommended to deprovision the cluster itself since we can’t provision a new control plane with the same cluster. For worker deprovisioning, we only need to run the worker script.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this blogpost we saw how to deploy a bare metal cluster once we have a Metal³(metal3-dev-env repo) deployed and by that point we will already have the nodes ready to be used for a bare metal cluster deployment.&lt;/p&gt;

&lt;p&gt;In the first section we show the various configuration files, templates, resource types and their meanings.Then we see the common steps involved in the provisioning process. After that we see a general overview of how all resources are related and at what point are they created - provision cluster/control plane/worker.&lt;/p&gt;

&lt;p&gt;In each of the provisioning sections we see the steps to monitor the provisioning and how to confirm if its successful or not, with brief explanations wherever required. Finally we see the deprovisioning section which uses the resource definitions generated at the time of provisioning to deprovision  cluster, control plane or worker.&lt;/p&gt;

&lt;p&gt;Here are a few resources which you might find useful if you want to explore further, some of them have already been shared earlier.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://metal3.io/&quot;&gt;Metal3-Documentation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://metal3.io/try-it.html&quot;&gt;Metal3-Try-it&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env&quot;&gt;Metal³/metal3-dev-env&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2020/02/18/metal3-dev-env-install-deep-dive.html&quot;&gt;Detailed metal3-dev-env walkthrough blogpost&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2020/02/27/talk-kubernetes-finland-metal3.html&quot;&gt;Kubernetes Metal3 Talk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/metal3-io/metal3-docs&quot;&gt;Metal3-Docs-github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Himanshu Roy</name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Cluster API provider renaming</title><link href="https://metal3.io/blog/2020/03/05/CAPI_provider_renaming.html" rel="alternate" type="text/html" title="Cluster API provider renaming" /><published>2020-03-05T00:00:00+00:00</published><updated>2020-03-05T00:00:00+00:00</updated><id>https://metal3.io/blog/2020/03/05/CAPI_provider_renaming</id><content type="html" xml:base="https://metal3.io/blog/2020/03/05/CAPI_provider_renaming.html">&lt;h2 id=&quot;renaming-of-cluster-api-provider&quot;&gt;Renaming of Cluster API provider&lt;/h2&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Backwards compatibility for v1alpha3&lt;/p&gt;&lt;p&gt;There is no backwards compatibility between v1alpha3 and v1alpha2 releases of
the Cluster API provider for Metal3.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the v1alpha3 release of Cluster API, the Metal3 provider was renamed from
&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cluster-api-provider-baremetal&lt;/code&gt; to &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cluster-api-provider-metal3&lt;/code&gt;. The Custom
Resource Definitions were also modified. This post dives into the changes.&lt;/p&gt;

&lt;h3 id=&quot;repository-renaming&quot;&gt;Repository renaming&lt;/h3&gt;

&lt;p&gt;From v1alpha3 onwards, the Cluster API provider will be developed in
&lt;a href=&quot;https://github.com/metal3-io/cluster-api-provider-metal3&quot;&gt;cluster-api-provider-metal3&lt;/a&gt;.
The v1alpha1 and v1alpha2 content will remain in
&lt;a href=&quot;https://github.com/metal3-io/cluster-api-provider-baremetal&quot;&gt;cluster-api-provider-baremetal&lt;/a&gt;.
This repository will be archived, but kept for the integration in metal3-dev-env.&lt;/p&gt;

&lt;h3 id=&quot;custom-resource-definition-modifications&quot;&gt;Custom Resource Definition modifications&lt;/h3&gt;

&lt;p&gt;The kind of the Custom Resource Definition (CRD) has been modified for the
following objects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalCluster&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal3Cluster&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;baremetalcluster&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3cluster&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalMachine&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal3Machine&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;baremetalmachine&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3machine&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalMachineTemplate&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Metal3MachineTemplate&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;baremetalmachinetemplate&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3machinetemplate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The custom resources deployed need to be modified accordingly.&lt;/p&gt;

&lt;h3 id=&quot;deployment-modifications&quot;&gt;Deployment modifications&lt;/h3&gt;

&lt;p&gt;The prefix of all deployed components for the Metal3 provider was modified
from &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capbm-&lt;/code&gt; to &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capm3-&lt;/code&gt;. The namespace in which the components are deployed by
default was modified from &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capbm-system&lt;/code&gt; to &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capm3-system&lt;/code&gt;.&lt;/p&gt;</content><author><name>Maël Kimmerlin</name></author><summary type="html">Renaming of Cluster API provider</summary></entry><entry><title type="html">Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup</title><link href="https://metal3.io/blog/2020/02/27/talk-kubernetes-finland-metal3.html" rel="alternate" type="text/html" title="Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin - Kubernetes and CNCF Finland Meetup" /><published>2020-02-27T00:00:00+00:00</published><updated>2020-02-27T00:00:00+00:00</updated><id>https://metal3.io/blog/2020/02/27/talk-kubernetes-finland-metal3</id><content type="html" xml:base="https://metal3.io/blog/2020/02/27/talk-kubernetes-finland-metal3.html">&lt;h2 id=&quot;conference-talk-metal-kubernetes-native-bare-metal-cluster-management---maël-kimmerlin&quot;&gt;Conference talk: Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin&lt;/h2&gt;

&lt;p&gt;On the 20th of January at the &lt;a href=&quot;https://www.meetup.com/Kubernetes-Finland/&quot;&gt;Kubernetes and CNCF Finland Meetup&lt;/a&gt;, Maël Kimmerlin gave a brilliant presentation about the status of the Metal³ project.&lt;/p&gt;

&lt;p&gt;In this presentation, Maël starts giving a short introduction of the &lt;a href=&quot;https://github.com/kubernetes-sigs/cluster-api&quot;&gt;Cluster API project&lt;/a&gt; which provides a solid foundation to develop the Metal³ Bare Metal Operator (BMO). The talk basically focuses on the &lt;strong&gt;v1alpha2&lt;/strong&gt; infrastructure provider features from the Cluster API.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;The video recording from the “Kubernetes and CNCF Finland Meetup” is composed by three talks. The video embedded starts with Maël’s talk.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;Playback of the video has been disabled by the author. Click on play button and then on “Watch this video on Youtube” link once it appears.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;iframe width=&quot;1110&quot; height=&quot;720&quot; style=&quot;height: 500px&quot; src=&quot;https://www.youtube.com/embed/3k5EfIQpw-E?t=4167&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;During the first part of the presentation, a detailed explanation of the different Kubernetes Custom Resource Definitions (CRDs) inside Metal³ is shown and also how they are linked with the Cluster API project. As an example, the image below shows the interaction between objects and controllers from both projects:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-27-talk-kubernetes-finland-metal3/metal3-crds-controllers.resized.png&quot; alt=&quot;crd v1alpha2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once finished the introductory part, Maël focuses on the main components of the Metal³ BMO and the provisioning process. This process starts with &lt;strong&gt;introspection&lt;/strong&gt;, where the bare metal server is registered by the operator. Then, the &lt;a href=&quot;https://docs.openstack.org/ironic-python-agent/latest/&quot;&gt;Ironic Python Agent&lt;/a&gt; (IPA) image is executed to collect all hardware information from the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-27-talk-kubernetes-finland-metal3/metal3-instrospection.resized.png&quot; alt=&quot;metal3 introspection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The second part of the process is the &lt;strong&gt;provisioning&lt;/strong&gt;. In this step, Maël explains how the Bare Metal Operator (BMO) is in charge along with Ironic to present the Operating System image to the physical server and complete its installation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-27-talk-kubernetes-finland-metal3/metal3-provisioning.resized.png&quot; alt=&quot;metal3 provisioning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Next, Maël deeply explains each Custom Resource (CR) used during the provisioning of target Kubernetes clusters in bare metal servers. He refers to objects such as &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cluster&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalCluster&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Machine&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalMachine&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHost&lt;/code&gt; and so on. Each one is clarified with a YAML file definition of a real case and a workflow diagram that shows the reconciliation procedure.&lt;/p&gt;

&lt;p&gt;Last part of the talk is dedicated to execute a demo where Maël creates a &lt;em&gt;target Kubernetes cluster&lt;/em&gt; from a running minikube VM (also called &lt;em&gt;bootstrap cluster&lt;/em&gt;) where Metal³ is deployed. As it is pointed out in the video, the demo is running in &lt;em&gt;emulated hardware&lt;/em&gt;. Actually, something similar to the &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env&quot;&gt;metal3-dev-env&lt;/a&gt; project which can be used to reproduce the demo. More information of the Metal³ development environment (metal3-dev-env) can be found in the &lt;a href=&quot;https://metal3.io/try-it.html&quot;&gt;Metal³ try-it section&lt;/a&gt;. In case you want to go deeper, take a look at the blog post &lt;a href=&quot;/blog/2020/02/18/metal3-dev-env-install-deep-dive.html&quot;&gt;A detailed walkthrough of the Metal³ development environment&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At the end, the result is a new Kubernetes cluster up and running. The cluster is deployed on two emulated physical servers: one runs as the control-plane node and the other as a worker node.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;The slides of the talk can be downloaded from &lt;a href=&quot;https://drive.google.com/open?id=1mdofzqIpH7XpFYkjB0ZC7EWU_RGW6aOl&quot;&gt;here&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/maelkimmerlin/&quot;&gt;Maël Kimmerlin&lt;/a&gt; Maël Kimmerlin is a Senior Software Engineer at Ericsson. In his own words:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I am an open-source enthusiast, focusing in Ericsson on Life Cycle Management of Kubernetes clusters on Bare Metal. I am very interested in the Cluster API project from the Kubernetes Lifecycle SIG, and active in its Bare Metal provider, that is Metal³, developing and encouraging the adoption of this project.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/3k5EfIQpw-E?t=4167&quot;&gt;Video: Metal³: Kubernetes Native Bare Metal Cluster Management&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=1mdofzqIpH7XpFYkjB0ZC7EWU_RGW6aOl&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alberto Losada</name></author><summary type="html">Conference talk: Metal³: Kubernetes Native Bare Metal Cluster Management - Maël Kimmerlin</summary></entry><entry><title type="html">A detailed walkthrough of the Metal³ development environment</title><link href="https://metal3.io/blog/2020/02/18/metal3-dev-env-install-deep-dive.html" rel="alternate" type="text/html" title="A detailed walkthrough of the Metal³ development environment" /><published>2020-02-18T10:09:00+00:00</published><updated>2020-02-18T10:09:00+00:00</updated><id>https://metal3.io/blog/2020/02/18/metal3-dev-env-install-deep-dive</id><content type="html" xml:base="https://metal3.io/blog/2020/02/18/metal3-dev-env-install-deep-dive.html">&lt;h2 id=&quot;introduction-to-metal3-dev-env&quot;&gt;&lt;strong&gt;Introduction to metal3-dev-env&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env&quot;&gt;metal3-dev-env&lt;/a&gt; is a collection of scripts in a Github repository inside the &lt;a href=&quot;https://github.com/metal3-io?type=source&quot;&gt;Metal³&lt;/a&gt; project that aims to allow contributors and other interested users to run a fully functional Metal³ environment for testing and have a first contact with the project. Actually, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; sets up an &lt;strong&gt;emulated environment&lt;/strong&gt; which creates a set of virtual machines (VMs) to manage as if they were bare metal hosts.&lt;/p&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;This is not an installation that is supposed to be run in production. Instead, it is focused on providing a development environment to test and validate new features.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; repository includes a set of scripts, libraries and resources used to set up a Metal³ development environment. On the &lt;a href=&quot;https://metal3.io/try-it.html&quot;&gt;Metal³ website&lt;/a&gt; there is already a documented process on how to use the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; scripts to set up a fully functional cluster to test the functionality of the Metal³ components.&lt;/p&gt;

&lt;p&gt;This procedure at 10,000 foot view is composed by 3 bash scripts plus a verification one:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;01_prepare_host.sh&lt;/strong&gt; - Mainly installs all needed packages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;02_configure_host.sh&lt;/strong&gt; - Basically create a set of VMs that will be managed as if they were bare metal hosts. It also downloads some images needed for Ironic.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;03_launch_mgmt_cluster.sh&lt;/strong&gt; - Launches a management cluster using minikube and runs the baremetal-operator on that cluster.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;04_verify.sh&lt;/strong&gt; - Finally runs a set of tests that verify that the deployment completed successfully&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this blog post we are going to expand the information and provide some hints and recommendations.&lt;/p&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;Metal³ project is changing rapidly, so probably this information is valuable in the short term. In any case, it is encouraged to double check that the information provided is still valid.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Before get down to it, it is worth defining the nomenclature used along the blog post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Host.&lt;/strong&gt; It is the server where the virtual environment is running. In this case it is a physical PowerEdge M520 with 2 x Intel(R) Xeon(R) CPU E5-2450 v2 @ 2.50GHz, 96GB RAM and a 140GB drive running CentOS 7 latest. Do not panic, lab environment should work with lower resources as well.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Virtual bare metal hosts.&lt;/strong&gt; These are the virtual machines (KVM based) that are running on the host which are emulating physical hosts in our lab. They are also called bare metal hosts even if they are not physical servers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Management or bootstrap cluster.&lt;/strong&gt; It is a fully functional Kubernetes cluster in charge of running all the necessary Metal³ operators and controllers to manage the infrastructure. In this case it is the minikube virtual machine.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Target cluster.&lt;/strong&gt; It is the Kubernetes cluster created from the management one. It is provisioned and configured using a native Kubernetes API for that purpose.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;create-the-metal-laboratory&quot;&gt;&lt;strong&gt;Create the Metal³ laboratory&lt;/strong&gt;&lt;/h2&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;A non-root user must exist in the host with password-less sudo access. This user is in charge of running the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; scripts.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;First thing that needs to be done is, obviously, cloning the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; repository:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1: ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/metal3-io/metal3-dev-env.git
Cloning into &lt;span class=&quot;s1&quot;&gt;'metal3-dev-env'&lt;/span&gt;...
remote: Enumerating objects: 22, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
remote: Counting objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;22/22&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
remote: Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;22/22&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
remote: Total 1660 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 8 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, pack-reused 1638
Receiving objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1660/1660&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, 446.08 KiB | 678.00 KiB/s, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
Resolving deltas: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;870/870&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before starting to deploy the Metal³ environment, it makes sense to detail a series of scripts inside the library folder that will be sourced in every step of the installation process. They are called &lt;em&gt;shared libraries&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1:~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; metal3-dev-env/lib/
common.sh
images.sh
logging.sh
network.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;shared-libraries&quot;&gt;Shared libraries&lt;/h3&gt;

&lt;p&gt;Although there are several scripts placed inside the lib folder that are sourced in some of the deployment steps, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;common.sh&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;logging.sh&lt;/code&gt; are the only ones used in all of the executions during the installation process.&lt;/p&gt;

&lt;h4 id=&quot;commonsh&quot;&gt;&lt;strong&gt;common.sh&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;First time this library is run, a new configuration file is created with several variables along with their default values. They will be used during the installation process. On the other hand, if the file already exists, then it just sources the values configured. The configuration file is created inside the cloned folder with &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;config_$USER&lt;/code&gt; as file name.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ls &lt;/span&gt;config_&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
config_alosadag.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The configuration file contains multiple variables that will be used during the set up. Some of them are detailed &lt;a href=&quot;https://metal3.io/try-it.html#setup&quot;&gt;in the setup section of the Metal³ try-it web page&lt;/a&gt;. In case you need to add or change global variables it should be done in this config file.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;I personally recommend modify or add variables in this config file instead of exporting them in the shell. By doing that, it is assured that they are persisted&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ~/metal3-dev-env/config_alosadag.sh
&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This is the subnet used on the &quot;baremetal&quot; libvirt network, created as the&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# primary network interface for the virtual bare metalhosts.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Default of 192.168.111.0/24 set in lib/common.sh&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#export EXTERNAL_SUBNET=&quot;192.168.111.0/24&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This SSH key will be automatically injected into the provisioned host&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# by the provision_host.sh script.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Default of ~/.ssh/id_rsa.pub is set in lib/common.sh&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#export SSH_PUB_KEY=~/.ssh/id_rsa.pub&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;common.sh&lt;/code&gt; library also makes sure there is a ssh public key available in the user’s ssh folder. This key will be injected by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cloud-init&lt;/code&gt; in all the virtual bare metal machines that will be configured later. Then, the user that executed the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; scripts is able to access the target cluster through ssh.&lt;/p&gt;

&lt;p&gt;Also, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;common.sh&lt;/code&gt; library also sets more global variables apart from the those in the config file. Note that these variables can be added to the config file along with the proper values for your environment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;SSH_KEY&lt;/td&gt;
      &lt;td&gt;${HOME}/.ssh/id_rsa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SSH_PUB_KEY&lt;/td&gt;
      &lt;td&gt;${SSH_KEY}.pub&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NUM_NODES&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VM_EXTRADISKS&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DOCKER_REGISTRY_IMAGE&lt;/td&gt;
      &lt;td&gt;docker.io/registry:latest&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VBMC_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/vbmc&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SUSHY_TOOLS_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/sushy-tools&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IPA_DOWNLOADER_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/ironic-ipa-downloader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IRONIC_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/ironic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IRONIC_INSPECTOR_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/ironic-inspector&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BAREMETAL_OPERATOR_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/baremetal-operator&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPI_VERSION&lt;/td&gt;
      &lt;td&gt;v1alpha1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPBM_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/cluster-api-provider-baremetal:v1alpha1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPBM_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/cluster-api-provider-baremetal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DEFAULT_HOSTS_MEMORY&lt;/td&gt;
      &lt;td&gt;8192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLUSTER_NAME&lt;/td&gt;
      &lt;td&gt;test1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KUBERNETES_VERSION&lt;/td&gt;
      &lt;td&gt;v1.17.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KUSTOMIZE_VERSION&lt;/td&gt;
      &lt;td&gt;v3.2.3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;It is important to mention that there are several basic functions defined in this file that will be used by the rest of scripts.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;loggingsh&quot;&gt;&lt;strong&gt;logging.sh&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;This script ensures that there is a log folder where all the information gathered during the execution of the scripts is stored. If there is any issue during the deployment, this is one of the first places to look at.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; logs/
01_prepare_host-2020-02-03-122452.log
01_prepare_host-2020-02-03-122956.log
host_cleanup-2020-02-03-122656.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;first-step-prepare-the-host&quot;&gt;&lt;strong&gt;First step: Prepare the host&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this first step (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;01_prepare_host.sh&lt;/code&gt;), the requirements needed to start the preparation of the host where the virtual bare metal hosts will run are fulfilled. Depending on the host’s operating system (OS), it will trigger a specific script for &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS/Red Hat&lt;/code&gt; or &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Ubuntu&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;note: “Note”
Currently &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS Linux 7&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Red Hat Enterprise Linux 8&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Ubuntu&lt;/code&gt; have been tested. &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/pull/157&quot;&gt;There is work in progress to adapt the deployment for CentOS Linux 8.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As stated previously, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS 7&lt;/code&gt; is the operating system chosen to run in both, the host and virtual servers. Therefore, specific packages of the operating system are applied in the following script:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;centos_install_requirements.sh&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This script enables &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;epel&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;tripleo&lt;/code&gt; (current-tripleo) repositories where several packages are installed: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;dnf&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ansible&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;wget&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;python3&lt;/code&gt; and python related packages such as &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;python-virtualbmc&lt;/code&gt; from tripleo repository.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;Notice that &lt;em&gt;SELinux&lt;/em&gt; is set to &lt;em&gt;permissive&lt;/em&gt; and an OS update is triggered, which will cause several packages to be upgraded since there are newer packages in the tripleo repositories (mostly python related) than in the rest of enabled repositories.
At this point, the container runtime is also installed. Note that by setting the variable &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CONTAINER_RUNTIME&lt;/code&gt; defined in &lt;a href=&quot;#commonsh&quot;&gt;common.sh&lt;/a&gt; is possible to choose between docker and podman, which is the default for CentOS. Remember that this behaviour can be overwritten in your config file.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Once the specific requirements for the elected operating system are accomplished, the download of several external artifacts is executed. Actually &lt;em&gt;minikube&lt;/em&gt;, &lt;em&gt;kubectl&lt;/em&gt; and &lt;em&gt;kustomize&lt;/em&gt; are downloaded from the internet. Notice that the version of Kustomize and Kubernetes are defined by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KUSTOMIZE_VERSION&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KUBERNETES_VERSION&lt;/code&gt; variables inside &lt;a href=&quot;#commonsh&quot;&gt;common.sh&lt;/a&gt;, but minikube is always downloading the latest version available.&lt;/p&gt;

&lt;p&gt;Next step deals with cleaning ironic containers and &lt;strong&gt;pods&lt;/strong&gt; that could be running in the host from failed deployments. This will ensure that there will be no issues when creating &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic-pod&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;infra-pod&lt;/code&gt; a little bit later in this first step.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;network.sh.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;At this point, the network library script is sourced. As expected, this library deals with the network configuration which includes: IP addresses, network definitions and IPv6 support which is disabled by default by setting &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;PROVISIONING_IPV6&lt;/code&gt; variable:&lt;/p&gt;

  &lt;blockquote&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;Name of the variable&lt;/th&gt;
        &lt;th&gt;Default value&lt;/th&gt;
        &lt;th&gt;Option&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;PROVISIONING_NETWORK&lt;/td&gt;
        &lt;td&gt;172.22.0.0/24&lt;/td&gt;
        &lt;td&gt;This is the subnet used to run the OS provisioning process&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;EXTERNAL_SUBNET&lt;/td&gt;
        &lt;td&gt;192.168.111.0/24&lt;/td&gt;
        &lt;td&gt;This is the subnet used on the “baremetal” libvirt network, created as the primary network interface for the virtual bare metal hosts&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;LIBVIRT_FIRMWARE&lt;/td&gt;
        &lt;td&gt;bios&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;PROVISIONING_IPV6&lt;/td&gt;
        &lt;td&gt;false&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Below it is depicted a network diagram of the different virtual networks and virtual servers involved in the Metal³ environment:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020-02-18-metal3-dev-env-install-deep-dive/metal3-dev-env.resized.png&quot; alt=&quot;metal³ dev env virtual networking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;images.sh.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;images.sh&lt;/code&gt; library file is sourced as well in script &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;01_prepare_host.sh&lt;/code&gt;. The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;images.sh&lt;/code&gt; script contains multiple variables that set the URL (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IMAGE_LOCATION&lt;/code&gt;), name (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IMAGE_NAME&lt;/code&gt;) and default username (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IMAGE_USERNAME&lt;/code&gt;) of the cloud image that needs to be downloaded. The values of each variable will differ depending on the operating system of the virtual bare metal hosts. Note that these images will be served from the host to the virtual servers through the provisioning network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;In our case, since &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS 7&lt;/code&gt; is the base operating system, values will be defined as:&lt;/p&gt;

  &lt;blockquote&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/th&gt;
        &lt;th&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;IMAGE_NAME&lt;/td&gt;
        &lt;td&gt;CentOS-7-x86_64-GenericCloud-1907.qcow2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;IMAGE_LOCATION&lt;/td&gt;
        &lt;td&gt;http://cloud.centos.org/centos/7/images&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;IMAGE USERNAME&lt;/td&gt;
        &lt;td&gt;centos&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;In case it is expected to use a custom cloud image, just modify the previous variables to match the right location.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now that the cloud image is defined, the download process can be started. First, a folder defined by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IRONIC_IMAGE_DIR&lt;/code&gt; should exist so that the image (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS-7-x86_64-GenericCloud-1907.qcow2&lt;/code&gt;) and its checksum can be stored. This folder and its content will be exposed through a local &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic&lt;/code&gt; container running in the host.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IRONIC_IMAGE_DIR&lt;/td&gt;
      &lt;td&gt;/opt/metal3-dev-env/ironic/html/images&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Below it is verified that the cloud image files were downloaded successfully in the defined folder:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ll /opt/metal3-dev-env/ironic/html/images
total 920324
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 alosadag alosadag 942407680 Feb  3 12:39 CentOS-7-x86_64-GenericCloud-1907.qcow2
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 alosadag alosadag        33 Feb  3 12:39 CentOS-7-x86_64-GenericCloud-1907.qcow2.md5sum
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the shared script &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;images.sh&lt;/code&gt; is sourced, the following container images are pre-cached locally to the host in order to speed up things later. Below it is shown the code snippet in charge of that task:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IMAGE_VAR &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;IRONIC_IMAGE IPA_DOWNLOADER_IMAGE VBMC_IMAGE SUSHY_TOOLS_IMAGE DOCKER_REGISTRY_IMAGE
+ &lt;span class=&quot;nv&quot;&gt;IMAGE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;quay.io/metal3-io/ironic
+ &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;podman pull quay.io/metal3-io/ironic
...
....
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The container image location of each one is defined by their respective variables:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;VBMC_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/vbmc&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SUSHY_TOOLS_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/sushy-tools&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IPA_DOWNLOADER_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/ironic-ipa-downloader&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IRONIC_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/ironic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DOCKER_REGISTRY_IMAGE&lt;/td&gt;
      &lt;td&gt;docker.io/registry:latest&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;In case it is expected to modify the public container images to test new features, it is worth mentioning that there is a container registry running as a privileged container in the host. Therefore it is recommended to upload your modified images there and just overwrite the previous variables to match the right location.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At this point, an Ansible role is run locally in order to complete the local configuration.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;ansible-playbook &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;working_dir=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$WORKING_DIR&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;virthost=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOSTNAME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; vm-setup/inventory.ini &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-vvv&lt;/span&gt; vm-setup/install-package-playbook.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This playbook imports two roles. One called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;packages_installation&lt;/code&gt;, which is in charge of installing a few more packages. The list of packages installed are listed as default Ansible variables &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/packages_installation/defaults/main.yml&quot;&gt;in the vm-setup role inside the metal3-dev-env repository&lt;/a&gt;. The other role is based on the &lt;a href=&quot;https://galaxy.ansible.com/fubarhouse/golang&quot;&gt;fubarhouse.golang&lt;/a&gt; Ansible Galaxy role. It is in charge of installing and configuring the exact &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;golang&lt;/code&gt; version &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;1.12.12&lt;/code&gt; defined in an Ansible variable in the &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/9fa752b90ed58fdadcd52c246d3023766dfcb2dc/vm-setup/install-package-playbook.yml#L12&quot;&gt;install-package-playbook.yml playbook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once the playbook is finished, a pod called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic-pod&lt;/code&gt; is created. Inside that pod, a &lt;em&gt;privileged&lt;/em&gt; &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic-ipa-downloader&lt;/code&gt; container is started and attached to the host network. This container is in charge of downloading the &lt;a href=&quot;https://docs.openstack.org/ironic-python-agent/latest/&quot;&gt;Ironic Python Agent&lt;/a&gt; (IPA) files to a shared volume defined by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;IRONIC_IMAGE_DIR&lt;/code&gt;. This folder is exposed by the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic&lt;/code&gt; container through HTTP.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;The &lt;a href=&quot;https://docs.openstack.org/ironic-python-agent/latest/&quot;&gt;Ironic Python Agent&lt;/a&gt; is an agent for controlling and deploying Ironic controlled baremetal nodes. Typically run in a ramdisk, the agent exposes a REST API for provisioning servers.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;See below the code snippet that fulfills the task:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;podman run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--net&lt;/span&gt; host &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ipa-downloader &lt;span class=&quot;nt&quot;&gt;--pod&lt;/span&gt; ironic-pod &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;IPA_BASEURI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /opt/metal3-dev-env/ironic:/shared quay.io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below, it is shown the status of the pods and containers at this point:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 metal3-dev-env]# podman pod list &lt;span class=&quot;nt&quot;&gt;--ctr-names&lt;/span&gt;
POD ID         NAME         STATUS    CREATED      CONTAINER INFO                                             INFRA ID
5a0d475351aa   ironic-pod   Running   6 days ago   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;5a0d475351aa-infra] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ipa-downloader]                      18f3a8f61407
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The process will wait until the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic-python-agent&lt;/code&gt; (IPA) initramfs, kernel and headers files are downloaded successfully. See below the files downloaded along with the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS 7&lt;/code&gt; cloud image:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ll /opt/metal3-dev-env/ironic/html/images
total 920324
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 alosadag alosadag 942407680 Feb  3 12:39 CentOS-7-x86_64-GenericCloud-1907.qcow2
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 alosadag alosadag        33 Feb  3 12:39 CentOS-7-x86_64-GenericCloud-1907.qcow2.md5sum
drwxr-xr-x. 2 root     root           147 Feb  3 12:41 ironic-python-agent-1862d000-59d9fdc6304b1
lrwxrwxrwx. 1 root     root            72 Feb  3 12:41 ironic-python-agent.initramfs -&amp;gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent.initramfs
lrwxrwxrwx. 1 root     root            69 Feb  3 12:41 ironic-python-agent.kernel -&amp;gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent.kernel
lrwxrwxrwx. 1 root     root            74 Feb  3 12:41 ironic-python-agent.tar.headers -&amp;gt; ironic-python-agent-1862d000-59d9fdc6304b1/ironic-python-agent.tar.headers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Afterwards, the script makes sure that libvirt is running successfully on the host and the non-privilege user has permissions to interact with it. Libvirt daemon should be running so that minikube can be installed successfully. See the following script snippet starting the minikube VM:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;su &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'minikube start --insecure-registry 192.168.111.1:5000'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; minikube v1.6.2 on Centos 7.7.1908
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Selecting &lt;span class=&quot;s1&quot;&gt;'kvm2'&lt;/span&gt; driver from user configuration &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;alternates: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;none]&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the same way as with the host, container images are pre-cached but in this case inside minikube local image repository. Notice that in this case the &lt;a href=&quot;https://github.com/metal3-io/baremetal-operator/&quot;&gt;Bare Metal operator&lt;/a&gt; (BMO) is also downloaded since it will run on minikube. The container location is defined by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BAREMETAL_OPERATOR_IMAGE&lt;/code&gt;. In case you want to test new features or new fixes to the BMO, just change the value of the variable to match the location of the modified image:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BAREMETAL_OPERATOR_IMAGE&lt;/td&gt;
      &lt;td&gt;quay.io/metal3-io/baremetal-operator&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;Remember that minikube is the management cluster in our environment. So it must run all the operators and controllers needed for Metal³.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Below it is shown the output of the script once all the container images have been pulled to minikube:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;su &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'minikube ssh sudo docker image ls'&lt;/span&gt; alosadag
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
quay.io/metal3-io/ironic                  latest              e5d81adf05ee        26 hours ago        693MB
quay.io/metal3-io/ironic-ipa-downloader   latest              d55b0dac2144        6 days ago          239MB
quay.io/metal3-io/ironic-inspector        latest              8bb5b844ada6        6 days ago          408MB
quay.io/metal3-io/baremetal-operator      latest              3c692a32ddd6        9 days ago          1.77GB
k8s.gcr.io/kube-proxy                     v1.17.0             7d54289267dc        7 weeks ago         116MB
k8s.gcr.io/kube-controller-manager        v1.17.0             5eb3b7486872        7 weeks ago         161MB
k8s.gcr.io/kube-scheduler                 v1.17.0             78c190f736b1        7 weeks ago         94.4MB
k8s.gcr.io/kube-apiserver                 v1.17.0             0cae8d5cc64c        7 weeks ago         171MB
kubernetesui/dashboard                    v2.0.0-beta8        eb51a3597525        7 weeks ago         90.8MB
k8s.gcr.io/coredns                        1.6.5               70f311871ae1        2 months ago        41.6MB
k8s.gcr.io/etcd                           3.4.3-0             303ce5db0e90        3 months ago        288MB
kubernetesui/metrics-scraper              v1.0.2              3b08661dc379        3 months ago        40.1MB
k8s.gcr.io/kube-addon-manager             v9.0.2              bd12a212f9dc        6 months ago        83.1MB
k8s.gcr.io/pause                          3.1                 da86e6ba6ca1        2 years ago         742kB
gcr.io/k8s-minikube/storage-provisioner   v1.8.1              4689081edb10        2 years ago         80.8MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the container images are stored, minikube can be stopped. In that moment, the virtual networks shown in the previous picture are attached to the minikube VM as it can be verified by the following command:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@smc-master metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;virsh domiflist minikube
Interface  Type       Source     Model       MAC
&lt;span class=&quot;nt&quot;&gt;-------------------------------------------------------&lt;/span&gt;
-          network    default    virtio      d4:38:25:25:c6:ca
-          network    minikube-net virtio    a4:c2:8a:9d:2a:d8
-          network    provisioning virtio    52:54:00:c8:50:97
-          network    baremetal  virtio      52:54:00:17:b4:ec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;At this point the host is ready to create the virtual infrastructure.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the video below it is exhibited all the configuration explained and executed during this &lt;em&gt;first&lt;/em&gt; step.&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;625&quot; style=&quot;height: 625px&quot; src=&quot;https://www.youtube.com/embed/VFbIHc3NbJo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;step-2-configure-the-host&quot;&gt;&lt;strong&gt;Step 2: Configure the host&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this step, the script &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;02_configure_host.sh&lt;/code&gt; basically configures the libvirt/KVM virtual infrastructure and starts services in the host that will be consumed by the virtual bare metal machines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Web server&lt;/code&gt; to expose the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic-python-agent&lt;/code&gt; (IPA) initramfs, kernel, headers and operating system cloud images.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Virtual BMC&lt;/code&gt; to emulate a real baseboard management controller (BMC).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Container registry&lt;/code&gt; where the virtual servers will pull the images needed to run a K8s installation.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;A baseboard management controller (BMC) is a specialized service processor that monitors the physical state of a computer, network server or other hardware device using sensors and communicating with the system administrator through an independent connection. The BMC is part of the Intelligent Platform Management Interface (IPMI) and is usually contained in the motherboard or main circuit board of the device to be monitored.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;First, a ssh-key in charge of communicating to libvirt is created if it does not exist previously. This key is called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;id_rsa_virt_power&lt;/code&gt;. It is added to the root authorized_keys and is used by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;sushy tools&lt;/code&gt; to contact libvirt.&lt;/p&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;sushy-tools&lt;/code&gt; is a set of simple simulation tools aiming at supporting the development and testing of the Redfish protocol implementations.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Next, another Ansible playbook called &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/setup-playbook.yml&quot;&gt;setup-playbook.yml&lt;/a&gt; is run against the host. It is focused on set up the virtual infrastructure around &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt;. Below it is shown the Ansible variables that are passed to the playbook, which actually are obtaining the values from the global variables defined in the &lt;a href=&quot;#commonsh&quot;&gt;common.sh&lt;/a&gt; or the configuration file.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;ANSIBLE_FORCE_COLOR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true &lt;/span&gt;ansible-playbook &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;working_dir=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$WORKING_DIR&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;num_nodes=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$NUM_NODES&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;extradisks=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$VM_EXTRADISKS&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;virthost=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOSTNAME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;platform=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$NODES_PLATFORM&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;libvirt_firmware=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$LIBVIRT_FIRMWARE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;default_memory=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DEFAULT_HOSTS_MEMORY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;manage_baremetal=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MANAGE_BR_BRIDGE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;provisioning_url_host=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PROVISIONING_URL_HOST&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; vm-setup/inventory.ini &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-vvv&lt;/span&gt; vm-setup/setup-playbook.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;WORKING_DIR&lt;/td&gt;
      &lt;td&gt;/opt/metal3-dev-env&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NUM_NODES&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VM_EXTRADISKS&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HOSTNAME&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;hostname&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NODES_PLATFORM&lt;/td&gt;
      &lt;td&gt;libvirt&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LIBVIRT_FIRMWARE&lt;/td&gt;
      &lt;td&gt;bios&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DEFAULT_HOSTS_MEMORY&lt;/td&gt;
      &lt;td&gt;8192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MANAGE_BR_BRIDGE&lt;/td&gt;
      &lt;td&gt;y&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PROVISIONING_URL_HOST&lt;/td&gt;
      &lt;td&gt;172.22.0.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;There are variables that are only defined as Ansible variables, e.g. number of CPUs of the virtual bare metal server, size of disks, etc. In case you would like to change properties not defined globally in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; take a look at the default variables specified in role: &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/common/defaults/main.yml&quot;&gt;common&lt;/a&gt; and &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/libvirt/defaults/main.yml&quot;&gt;libvirt&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;setup-playbook.yml&lt;/code&gt; is composed by 3 roles, which are detailed below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Common.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This role sets up the virtual hardware and network configuration of the VMs. Actually it is a &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/libvirt/meta/main.yml&quot;&gt;dependency&lt;/a&gt; of the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;libvirt&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtbmc&lt;/code&gt; Ansible roles. This means that the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;common&lt;/code&gt; role must always be executed before the roles that depend on them. Also, they are only executed once. If two roles state the same one as their dependency, it is only executed the first time.&lt;/p&gt;

  &lt;blockquote&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Libvirt.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;It actually is the role that configures the virtual bare metal servers. They are all identically defined with the same hardware and network configuration. Note that they are not started since they will be booted later by ironic during the provisioning process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;It is possible to change the number of VMs to provision by replacing the value of &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;NUMBER_NODES&lt;/code&gt;&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Finally, once the VMs are defined and we have their MAC address, the ironic inventory file &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic_nodes_json&lt;/code&gt; is created. The action of creating a node is part of the enrollment process and the first step to prepare a node to reach the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;available&lt;/code&gt; status.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;nodes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;node-0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;driver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ipmi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;resource_class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;baremetal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;driver_info&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;username&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;admin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;port&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;6230&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ipmi://192.168.111.1:6230&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;deploy_kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.22.0.1/images/ironic-python-agent.kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;deploy_ramdisk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.22.0.1/images/ironic-python-agent.initramfs&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ports&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;00:00:e0:4b:24:8b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;pxe_enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;properties&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;local_gb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;50&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cpu_arch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;x86_64&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;node-1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;driver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ipmi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;resource_class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;baremetal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;driver_info&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;username&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;admin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;port&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;6231&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ipmi://192.168.111.1:6231&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;deploy_kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.22.0.1/images/ironic-python-agent.kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;deploy_ramdisk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.22.0.1/images/ironic-python-agent.initramfs&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ports&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;00:00:e0:4b:24:8f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;pxe_enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;properties&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;local_gb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;50&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;cpu_arch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;x86_64&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;This role is also used to tear down the virtual infrastructure depending on the variable &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/2b5d8e76f33d143757d1f0b9b1e82dc662245b9c/vm-setup/roles/libvirt/defaults/main.yml#L2&quot;&gt;libvirt_action&lt;/a&gt; inside the Ansible role: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;setup or teardown&lt;/code&gt;.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;VirtBMC&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This role is only executed if the bare metal virtual machines are created in libvirt, because &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; needs libvirt to emulate a real BMC.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;VirtualBMC (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vmbc&lt;/code&gt;) tool simulates a Baseboard Management Controller (BMC) by exposing IPMI responder to the network and talking to libvirt at the host vBMC is running at. Basically, manipulate virtual machines which pretend to be bare metal servers.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;The &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;virtbmc&lt;/code&gt; Ansible role creates the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;sushy-tools&lt;/code&gt; configuration in the host for each virtual bare metal nodes. Note that each virtual bare metal host will have a different &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; socket exposed in the host. The communication to each &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; is needed by the BMO to start, stop, configure the boot order, etc during the provisioning stage. Finally, this folders containing the configuration will be mounted by the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;sushy-tools&lt;/code&gt; containers.&lt;/p&gt;

  &lt;blockquote&gt;

  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--color&lt;/span&gt; /opt/metal3-dev-env/virtualbmc
total 0
drwxr-x---. 2 root root 21 Feb  5 11:07 sushy-tools
drwxr-x---. 4 root root 70 Feb  5 11:08 vbmc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;
Next, both host provisioning and baremetal interfaces are configured. The provisioning interface, as the name suggests, will be used to provision the virtual bare metal hosts by means of the `Bare Metal Operator`. This interface is configured with an static IP (172.22.0.1):

```sh
[alosadag@smc-master metal3-dev-env]$ ifconfig provisioning
provisioning: flags=4163&lt;span class=&quot;nt&quot;&gt;&amp;lt;UP&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;BROADCAST&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;RUNNING&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;MULTICAST&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;  mtu 1500
    inet 172.22.0.1  netmask 255.255.255.0  broadcast 172.22.0.255
    inet6 fe80::1091:c1ff:fea1:6a0f  prefixlen 64  scopeid 0x20&lt;span class=&quot;nt&quot;&gt;&amp;lt;link&amp;gt;&lt;/span&gt;
    ether 12:91:c1:a1:6a:0f  txqueuelen 1000  (Ethernet)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the other hand, the baremetal virtual interface behaves as an external network. This interface is able to reach the internet and it is the network where the different Kubernetes nodes will exchange information. This interface is configured as auto, so the IP is retrieved by DHCP.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@smc-master metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ifconfig baremetal
baremetal: &lt;span class=&quot;nv&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4099&amp;lt;UP,BROADCAST,MULTICAST&amp;gt;  mtu 1500
    inet 192.168.111.1  netmask 255.255.255.0  broadcast 192.168.111.255
    ether 52:54:00:db:85:29  txqueuelen 1000  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Ethernet&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, an Ansible role called &lt;a href=&quot;https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/firewall.yml&quot;&gt;firewall&lt;/a&gt; will be executed targetting the host to be sure that the proper ports are opened. In case your host is running &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Red Hat Enterprise Linux&lt;/code&gt; or &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CentOS 8&lt;/code&gt;, firewalld module will be used. In any other case, iptables module is the choice.&lt;/p&gt;

&lt;p&gt;Below, the code snippet where &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;firewalld&lt;/code&gt; or &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;iptables&lt;/code&gt; is assigned:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Use firewalld on CentOS/RHEL, iptables everywhere else&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;USE_FIREWALLD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;False
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$OS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;rhel&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$OS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;centos&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OS_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; 8 &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;then
 &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;USE_FIREWALLD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;True
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;This behaviour can be changed by replacing the value of the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;USE_FIREWALLD&lt;/code&gt; variable&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The ports managed by this role are all associated to the services that take part of the provisioning process: &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ironic&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;httpd&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;pxe&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;container registry&lt;/code&gt;..&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;Services like ironic, pxe, keepalived, httpd and the container registry are running in the host as containers attached to the host network on the host’s provisioning interface. On the other hand, the vbmc service is also running as a privileged container and it is listening in the host’s baremetal interface.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Once the network is configured, a local &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;container registry&lt;/code&gt; is started. It will be needed in the case of using local built images. In that case, the container images can be modified locally and pushed to the local registry. At that point, the specific image location variable must be changed so it must point out the local registry. This process makes easy to verify and test changes to the code locally.&lt;/p&gt;

&lt;p&gt;At this point the following containers are running inside two pods on the host: &lt;em&gt;infra-pod&lt;/em&gt; and &lt;em&gt;ironic-pod&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 metal3-dev-env]# podman pod list &lt;span class=&quot;nt&quot;&gt;--ctr-names&lt;/span&gt;
POD ID         NAME         STATUS    CREATED      CONTAINER INFO                                             INFRA ID
67cc53713145   infra-pod    Running   6 days ago   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;vbmc] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;sushy-tools] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;httpd-infra] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;67cc53713145-infra]    f1da23fcd77f
5a0d475351aa   ironic-pod   Running   6 days ago   &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;5a0d475351aa-infra] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ipa-downloader]                      18f3a8f61407
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are detailed the containers inside the &lt;em&gt;infra-pod&lt;/em&gt; pod which are running as privileged using the host network:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;The httpd container.&lt;/strong&gt; &amp;gt; &amp;gt;
A folder called &lt;em&gt;shared&lt;/em&gt; where the cloud OS image and IPA files are available is mounted and exposed to the virtual bare metal hosts.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;sudo podman run -d –net host –privileged –name httpd-infra –pod infra-pod -v /opt/metal3-dev-env/ironic:/shared –entrypoint /bin/runhttpd quay.io/metal3-io/ironic&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;
&amp;gt; This folder also contains the `inspector.ipxe` file which contains the information needed to be able to run the `ironic-python-agent` kernel and initramfs. Below, httpd-infra container is accessed and it has been verified that host's `/opt/metal3-dev-env/ironic/` (`IRONIC_DATA_DIR`) is mounted inside the *shared* folder of the container:

```sh
[alosadag@eko1 metal3-dev-env]$ sudo podman exec -it httpd-infra bash
[root@infra-pod shared]# cat html/inspector.ipxe
#!ipxe

:retry_boot
echo In inspector.ipxe
imgfree
# NOTE(dtantsur): keep inspection kernel params in [mdns]params in ironic-inspector-image
kernel --timeout 60000 http://172.22.0.1:80/images/ironic-python-agent.kernel ipa-inspection-callback-url=http://172.22.0.1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd.journald.forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-dhcp-all-interfaces=1 ipa-collect-lldp=1 initrd=ironic-python-agent.initramfs || goto retry_boot
initrd --timeout 60000 http://172.22.0.1:80/images/ironic-python-agent.initramfs || goto retry_boot
boot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;The vbmc container.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This container mounts two host folders: one is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;/opt/metal3-dev-env/virtualbmc/vbmc&lt;/code&gt; where &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; configuration for each node is stored, the other folder is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;/root/.ssh&lt;/code&gt; where root keys are located, specifically &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;id_rsa_virt_power&lt;/code&gt; which is used to manage the communication with libvirt.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;podman run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--net&lt;/span&gt; host &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; vbmc &lt;span class=&quot;nt&quot;&gt;--pod&lt;/span&gt; infra-pod &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /opt/metal3-dev-env/virtualbmc/vbmc:/root/&amp;gt; .vbmc &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /root/.ssh:/root/ssh quay.io/metal3-io/vbmc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;The sushy-tools container.&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;This container mounts the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;/opt/metal3-dev-env/virtualbmc/sushy-tools config folder and the&lt;/code&gt;/root/.ssh&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;local folder as well. The functionality is similar as the&lt;/code&gt;vbmc`, however this use redfish instead of ipmi to connect to the BMC.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;podman run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--net&lt;/span&gt; host &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; sushy-tools &lt;span class=&quot;nt&quot;&gt;--pod&lt;/span&gt; infra-pod &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /opt/metal3-dev-env/virtualbmc/&amp;gt; sushy-tools:/root/sushy &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /root/.ssh:/root/ssh quay.io/metal3-io/sushy-tools
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;At this point the virtual infrastructure must be ready to apply the Kubernetes specific configuration. Note that all the VMs specified by &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;NUMBER_NODES&lt;/code&gt; and minikube must be shut down and the defined virtual network must be active:&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@smc-master metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;virsh list &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt;
 Id    Name                           State
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------&lt;/span&gt;
 -     minikube                       shut off
 -     node_0                         shut off
 -     node_1                         shut off
 -     node_2                         shut off

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@smc-master metal3-dev-env]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;virsh net-list &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt;
 Name                 State      Autostart     Persistent
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------&lt;/span&gt;
 baremetal            active     &lt;span class=&quot;nb&quot;&gt;yes           yes
 &lt;/span&gt;default              active     &lt;span class=&quot;nb&quot;&gt;yes           yes
 &lt;/span&gt;minikube-net         active     &lt;span class=&quot;nb&quot;&gt;yes           yes
 &lt;/span&gt;provisioning         active     &lt;span class=&quot;nb&quot;&gt;yes           yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the video below it is exhibited all the configuration explained and executed during this &lt;em&gt;second&lt;/em&gt; step.&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;625&quot; style=&quot;height: 625px&quot; src=&quot;https://www.youtube.com/embed/m8gTRZWEvc8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;step-3-launch-the-management-cluster-minikube&quot;&gt;&lt;strong&gt;Step 3: Launch the management cluster (minikube)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The third script called &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;03_launch_mgmt_cluster.sh&lt;/code&gt; basically configures minikube to become a Metal³ management cluster. On top of minikube the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;baremetal-operator&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capi-controller-manager&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;capbm-controller-manager&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;cabpk-controller-manager&lt;/code&gt; are installed in the metal3 namespace.&lt;/p&gt;

&lt;p&gt;In a more detailed way, the script clones the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Bare Metal Operator&lt;/code&gt; (&lt;a href=&quot;https://github.com/metal3-io/baremetal-operator&quot;&gt;BMO&lt;/a&gt;) and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Cluster API Provider for Managed Bare Metal Hardware operator&lt;/code&gt; (&lt;a href=&quot;https://github.com/metal3-io/cluster-api-provider-baremetal&quot;&gt;CAPBM&lt;/a&gt;) git repositories, creates the cloud.yaml file and starts the minikube virtual machine. Once minikube is up and running, the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BMO&lt;/code&gt; is built and executed in minikube’s Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;In case of the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;Bare Metal Operator&lt;/code&gt;, the branch by default to clone is master, however this and other variables shown in the following table can be replaced in the config file:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ &lt;span class=&quot;nv&quot;&gt;BMOREPO&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://github.com/metal3-io/baremetal-operator.git
+ &lt;span class=&quot;nv&quot;&gt;BMOBRANCH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Name of the variable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Options&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;BMOREPO&lt;/td&gt;
      &lt;td&gt;https://github.com/metal3-io/baremetal-operator.git&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BMOBRANCH&lt;/td&gt;
      &lt;td&gt;master&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPBMREPO&lt;/td&gt;
      &lt;td&gt;https://github.com/metal3-io/cluster-api-provider-baremetal.git&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPI_VERSION&lt;/td&gt;
      &lt;td&gt;v1alpha2&lt;/td&gt;
      &lt;td&gt;v1alpha1 or v1alpha3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FORCE_REPO_UPDATE&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BMO_RUN_LOCAL&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CAPBM_RUN_LOCAL&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Once the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BMO&lt;/code&gt; variables are configured, it is time for the operator to be deployed using &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kustomize&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; as it can seen from the logs:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;a href=&quot;https://github.com/kubernetes-sigs/kustomize&quot;&gt;Kustomize&lt;/a&gt; is a Kubernetes tool that lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ kustomize build bmo-dirPrHIrcl
+ kubectl apply &lt;span class=&quot;nt&quot;&gt;-f-&lt;/span&gt;
namespace/metal3 created
customresourcedefinition.apiextensions.k8s.io/baremetalhosts.metal3.io created
serviceaccount/metal3-baremetal-operator created
clusterrole.rbac.authorization.k8s.io/metal3-baremetal-operator created
clusterrolebinding.rbac.authorization.k8s.io/metal3-baremetal-operator created
configmap/ironic-bmo-configmap-75tkt49k5c created
secret/mariadb-password-d88m524c46 created
deployment.apps/metal3-baremetal-operator created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BMO&lt;/code&gt; objects are applied, it’s time to transform the virtual bare metal hosts information into a yaml file of kind &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHost&lt;/code&gt; Custom Resource (CR). This is done by a golang script passing them the IPMI address, BMC username and password, which are stored as a Kubernetes secret, MAC address and name:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ go run /home/alosadag/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go &lt;span class=&quot;nt&quot;&gt;-address&lt;/span&gt; ipmi://192.168.111.1:6230 &lt;span class=&quot;nt&quot;&gt;-password&lt;/span&gt; password &lt;span class=&quot;nt&quot;&gt;-user&lt;/span&gt; admin &lt;span class=&quot;nt&quot;&gt;-boot-mac&lt;/span&gt; 00:be:bc:fd:17:f3 node-0
+ &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; name address user password mac
+ go run /home/alosadag/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go &lt;span class=&quot;nt&quot;&gt;-address&lt;/span&gt; ipmi://192.168.111.1:6231 &lt;span class=&quot;nt&quot;&gt;-password&lt;/span&gt; password &lt;span class=&quot;nt&quot;&gt;-user&lt;/span&gt; admin &lt;span class=&quot;nt&quot;&gt;-boot-mac&lt;/span&gt; 00:be:bc:fd:17:f7 node-1
+ &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; name address user password mac
+ go run /home/alosadag/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go &lt;span class=&quot;nt&quot;&gt;-address&lt;/span&gt; ipmi://192.168.111.1:6232 &lt;span class=&quot;nt&quot;&gt;-password&lt;/span&gt; password &lt;span class=&quot;nt&quot;&gt;-user&lt;/span&gt; admin &lt;span class=&quot;nt&quot;&gt;-boot-mac&lt;/span&gt; 00:be:bc:fd:17:fb node-2
+ &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; name address user password mac
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is shown the bare metal host definition of node-1. Note that the IPMI address is the IP of the host’s provisioning interface. Behind the scenes, IPMI is handled by the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vbmc&lt;/code&gt; container running in the host.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Secret&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node-1-bmc-secret&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Opaque&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;YWRtaW4=&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cGFzc3dvcmQ=&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metal3.io/v1alpha1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BareMetalHost&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node-1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;online&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;bootMACAddress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;00:00:e0:4b:24:8f&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;bmc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ipmi://192.168.111.1:6231&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;credentialsName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node-1-bmc-secret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See that the MAC address configured in the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHost&lt;/code&gt; spec definition matches &lt;em&gt;node-1&lt;/em&gt; provisioning interface:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@eko1 metal3-dev-env]# virsh domiflist node_1
Interface  Type       Source     Model       MAC
&lt;span class=&quot;nt&quot;&gt;-------------------------------------------------------&lt;/span&gt;
vnet4      bridge     provisioning virtio      00:00:e0:4b:24:8f
vnet5      bridge     baremetal  virtio      00:00:e0:4b:24:91
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the script apply in namespace metal3 each of the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHost&lt;/code&gt; yaml files that match each virtual bare metal host:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; bmhosts_crs.yaml &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
secret/node-0-bmc-secret created
baremetalhost.metal3.io/node-0 created
secret/node-1-bmc-secret created
baremetalhost.metal3.io/node-1 created
secret/node-2-bmc-secret created
baremetalhost.metal3.io/node-2 created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lastly, it is the turn of the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CAPBM&lt;/code&gt;. Similar to &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BMO&lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kustomize&lt;/code&gt; is used to create the different Kubernetes components and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; applied the files into the management cluster.&lt;/p&gt;

&lt;div class=&quot;premonition warning&quot;&gt;&lt;div class=&quot;fa fa-exclamation-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Warning&lt;/p&gt;&lt;p&gt;Note that installing &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;CAPBM&lt;/code&gt; includes installing the components of the &lt;a href=&quot;https://github.com/kubernetes-sigs/cluster-api&quot;&gt;Cluster API&lt;/a&gt; and the components of the &lt;a href=&quot;https://github.com/kubernetes-sigs/cluster-api/tree/master/bootstrap/kubeadm&quot;&gt;Cluster API bootstrap provider kubeadm&lt;/a&gt; (CABPK)&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Below the objects are created through the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;generate.sh&lt;/code&gt; script:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;++ &lt;span class=&quot;nb&quot;&gt;mktemp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; capbm-XXXXXXXXXX
+ &lt;span class=&quot;nv&quot;&gt;kustomize_overlay_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;capbm-eJPOjCPASD

+ ./examples/generate.sh &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt;
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/cluster.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/controlplane.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3crds.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/metal3plane.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/machinedeployment.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-cluster-api.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-kubeadm.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/provider-components/provider-components-baremetal.yaml
Generated /home/alosadag/go/src/github.com/metal3-io/cluster-api-provider-baremetal/examples/_out/provider-components.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kustomize&lt;/code&gt; configures the files accordingly to the values defined and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; applies them:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;+ kustomize build capbm-eJPOjCPASD
+ kubectl apply &lt;span class=&quot;nt&quot;&gt;-f-&lt;/span&gt;
namespace/cabpk-system created
namespace/capbm-system created
namespace/capi-system created
customresourcedefinition.apiextensions.k8s.io/baremetalclusters.infrastructure.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/baremetalmachines.infrastructure.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/baremetalmachinetemplates.infrastructure.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/clusters.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/kubeadmconfigs.bootstrap.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/kubeadmconfigtemplates.bootstrap.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/machinedeployments.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/machines.cluster.x-k8s.io created
customresourcedefinition.apiextensions.k8s.io/machinesets.cluster.x-k8s.io created
role.rbac.authorization.k8s.io/cabpk-leader-election-role created
role.rbac.authorization.k8s.io/capbm-leader-election-role created
role.rbac.authorization.k8s.io/capi-leader-election-role created
clusterrole.rbac.authorization.k8s.io/cabpk-manager-role created
clusterrole.rbac.authorization.k8s.io/cabpk-proxy-role created
clusterrole.rbac.authorization.k8s.io/capbm-manager-role created
clusterrole.rbac.authorization.k8s.io/capbm-proxy-role created
clusterrole.rbac.authorization.k8s.io/capi-manager-role created
rolebinding.rbac.authorization.k8s.io/cabpk-leader-election-rolebinding created
rolebinding.rbac.authorization.k8s.io/capbm-leader-election-rolebinding created
rolebinding.rbac.authorization.k8s.io/capi-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/cabpk-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/cabpk-proxy-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/capbm-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/capbm-proxy-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/capi-manager-rolebinding created
secret/capbm-webhook-server-secret created
service/cabpk-controller-manager-metrics-service created
service/capbm-controller-manager-service created
service/capbm-controller-metrics-service created
deployment.apps/cabpk-controller-manager created
deployment.apps/capbm-controller-manager created
deployment.apps/capi-controller-manager created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;At this point all controllers and operators must be running in the namespace metal3 of the management cluster (minikube). All virtual bare metal hosts configured must be shown as &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHosts&lt;/code&gt; resources in the metal3 namespace as well. They should be in ready status and stopped (online is false)&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the video below it is exhibited all the configuration explained and executed during this &lt;em&gt;third&lt;/em&gt; step.&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;625&quot; style=&quot;height: 625px&quot; src=&quot;https://www.youtube.com/embed/GaSmL7xrYNs&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;step-4-verification&quot;&gt;&lt;strong&gt;Step 4: Verification&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The last script &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;04_verify.sh&lt;/code&gt; is in charge of verifying that the deployment has been successful by checking several things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Custom resources (CR) and custom resource definition (CRD) were applied and exist in the cluster.&lt;/li&gt;
  &lt;li&gt;Verify that the virtual bare metal hosts matches the information detailed in the&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHost&lt;/code&gt; object.&lt;/li&gt;
  &lt;li&gt;All containers are in running status.&lt;/li&gt;
  &lt;li&gt;Verify virtual network configuration and status.&lt;/li&gt;
  &lt;li&gt;Verify operators and controllers are running.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, this verification can be easily achieved manually. For instance, checking that controllers and operators running in the management cluster (minikube) and all the virtual bare metal hosts are in ready status:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3 &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
cabpk-controller-manager-5c67dd56c4-wfwbh    2/2     Running   9          6d23h   172.17.0.5       minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
capbm-controller-manager-7f9b8f96b7-grl4r    2/2     Running   12         6d23h   172.17.0.4       minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
capi-controller-manager-798c76675f-dxh2n     1/1     Running   10         6d23h   172.17.0.6       minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
metal3-baremetal-operator-5b4c59755d-h4zkp   6/6     Running   8          6d23h   192.168.39.101   minikube   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify that the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;BareMetalHosts&lt;/code&gt; provisioning status is &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ready&lt;/code&gt; and the BMC configuration is correct. Check that all virtual bare metal hosts are shut down (online is false):&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get baremetalhosts &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; metal3
NAME     STATUS   PROVISIONING STATUS   CONSUMER             BMC                         HARDWARE PROFILE   ONLINE   ERROR
node-0   OK       ready                                      ipmi://192.168.111.1:6230   unknown            &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;node-1   OK       ready                                      ipmi://192.168.111.1:6231   unknown            &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;node-2   OK       ready                                      ipmi://192.168.111.1:6232   unknown            &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Get the list of CRDs created in the cluster. Check that, at least, the following ones exist:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@eko1 ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get crds
NAME                                                        CREATED AT
baremetalclusters.infrastructure.cluster.x-k8s.io           2020-01-22T13:19:42Z
baremetalhosts.metal3.io                                    2020-01-22T13:19:35Z
baremetalmachines.infrastructure.cluster.x-k8s.io           2020-01-22T13:19:42Z
baremetalmachinetemplates.infrastructure.cluster.x-k8s.io   2020-01-22T13:19:42Z
clusters.cluster.x-k8s.io                                   2020-01-22T13:19:42Z
kubeadmconfigs.bootstrap.cluster.x-k8s.io                   2020-01-22T13:19:42Z
kubeadmconfigtemplates.bootstrap.cluster.x-k8s.io           2020-01-22T13:19:42Z
machinedeployments.cluster.x-k8s.io                         2020-01-22T13:19:43Z
machines.cluster.x-k8s.io                                   2020-01-22T13:19:43Z
machinesets.cluster.x-k8s.io                                2020-01-22T13:19:43Z
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition info&quot;&gt;&lt;div class=&quot;fa fa-info-circle&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Information&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;KUBECONFIG&lt;/code&gt; file is stored in the user’s home directory (~/.kube/config) that executed the scripts.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Check the status of all the applications running in minikube or better said, in the management cluster.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;syntax&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;alosadag@smc-master logs]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt;
NAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE
kube-system   coredns-6955765f44-fkdzp                    1/1     Running   1          164m
kube-system   coredns-6955765f44-fxzvz                    1/1     Running   1          164m
kube-system   etcd-minikube                               1/1     Running   1          164m
kube-system   kube-addon-manager-minikube                 1/1     Running   1          164m
kube-system   kube-apiserver-minikube                     1/1     Running   1          164m
kube-system   kube-controller-manager-minikube            1/1     Running   1          164m
kube-system   kube-proxy-87g98                            1/1     Running   1          164m
kube-system   kube-scheduler-minikube                     1/1     Running   1          164m
kube-system   storage-provisioner                         1/1     Running   2          164m
metal3        cabpk-controller-manager-5c67dd56c4-rldk4   2/2     Running   0          156m
metal3        capbm-controller-manager-7f9b8f96b7-mdfcw   2/2     Running   0          156m
metal3        capi-controller-manager-84947c7497-k6twl    1/1     Running   0          156m
metal3        metal3-baremetal-operator-78bffc8d-z5hqs    6/6     Running   0          156m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the video below it is exhibited all the configuration explained and executed during the &lt;em&gt;verification&lt;/em&gt; steps.&lt;/p&gt;

&lt;iframe width=&quot;1110&quot; height=&quot;625&quot; style=&quot;height: 625px&quot; src=&quot;https://www.youtube.com/embed/8LilwO-_WzU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this post a deep dive into the &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal3-dev-env&lt;/code&gt; scripts was shown. It has been deeply detailed the process of creating a Metal³ &lt;strong&gt;emulated environment&lt;/strong&gt; from a set of virtual machines (VMs) to manage as if they were bare metal hosts.&lt;/p&gt;

&lt;p&gt;After this post, the reader should have acquired a basic understanding of all the pieces involved in the Metal³ project. Also, and more important, how these scripts can be adapted to your specific needs. Remember that this can be achieved in multiple ways: replacing values in the global variables, replacing Ansible default variables or even modifying playbooks or the scripts themselves.&lt;/p&gt;

&lt;p&gt;Notice that the Metal³ development environment also focuses on developing new features of the BMO or CAPBM and being able to test them locally.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL2y-qnqBbesZZQKyKbuI6vIVkPrCPuK9T&quot;&gt;Video playlist: A detailed walkthrough the installation of the metal3-dev-env on Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://metal3.io/try-it.html&quot;&gt;Getting started with Metal3.io&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/metal3-io?type=source&quot;&gt;Metal³ code repositories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alberto Losada</name></author><summary type="html">Introduction to metal3-dev-env</summary></entry><entry><title type="html">Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019</title><link href="https://metal3.io/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html" rel="alternate" type="text/html" title="Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla - Shift Dev 2019" /><published>2020-01-20T07:10:00+00:00</published><updated>2020-01-20T07:10:00+00:00</updated><id>https://metal3.io/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal</id><content type="html" xml:base="https://metal3.io/blog/2020/01/20/metal3_deploy_kubernetes_on_bare_metal.html">&lt;h2 id=&quot;conference-talk-metal-deploy-kubernetes-on-bare-metal---yolanda-robla-red-hat&quot;&gt;Conference talk: Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat&lt;/h2&gt;

&lt;p&gt;Some of the most influential minds in the developer industry were landing in the gorgeous ancient city of Split, Croatia, to talk in the &lt;a href=&quot;https://dev.shiftconf.co&quot;&gt;Shift Dev 2019 - Developer Conference&lt;/a&gt; about the most cutting edge technologies, techniques and biggest trends in the developer space.&lt;/p&gt;

&lt;p&gt;In this video, Yolanda Robla speaks about the deployment of Kubernetes on Bare Metal with the help of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/iHlaimz48vg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/yolanda-robla-2008158/&quot;&gt;Yolanda Robla&lt;/a&gt; Yolanda Robla is a Principal Software Engineer at Red Hat. In her own words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In my current position in Red Hat as an NFV Partner Engineer, I investigate new technologies and create proofs of concept for partners to embrace new technologies. Being the current PTL of Akraino, I am involved in designing and implementing systems based on Kubernetes for the Edge use cases, ensuring high scalability and reproducibility using a GitOps approach.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iHlaimz48vg&amp;amp;t=8s&quot;&gt;Video: Metal³: Deploy Kubernetes on Bare Metal video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><summary type="html">Conference talk: Metal³: Deploy Kubernetes on Bare Metal - Yolanda Robla, Red Hat</summary></entry><entry><title type="html">Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp;amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019</title><link href="https://metal3.io/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html" rel="alternate" type="text/html" title="Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp; Doug Hellmann, Red Hat - KubeCon NA, November 2019" /><published>2019-12-04T10:09:00+00:00</published><updated>2019-12-04T10:09:00+00:00</updated><id>https://metal3.io/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management</id><content type="html" xml:base="https://metal3.io/blog/2019/12/04/Introducing_metal3_kubernetes_native_bare_metal_host_management.html">&lt;h2 id=&quot;conference-talk-introducing-metal-kubernetes-native-bare-metal-host-management---russell-bryant--doug-hellmann-red-hat&quot;&gt;Conference talk: Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp;amp; Doug Hellmann, Red Hat&lt;/h2&gt;

&lt;p&gt;Metal³ (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;metal cubed/Kube&lt;/code&gt;) is a new open source bare metal host provisioning tool created to enable Kubernetes-native infrastructure management. Metal³ enables the management of bare metal hosts via custom resources managed through the Kubernetes API as well as the monitoring of bare metal host metrics to Prometheus. This presentation will explain the motivations behind creating the project and what has been accomplished so far. This will be followed by an architectural overview and description of the Custom Resource Definitions (CRDs) for describing bare metal hosts, leading to a demonstration of using Metal³ in a Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;In this video, Russell Bryant and Doug Hellmann speak about the what’s and how’s of Metal³, a new tool that enables the management of bare metal hosts via custom resources managed through the Kubernetes API.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; style=&quot;height: 315px&quot; src=&quot;https://www.youtube.com/embed/KIIkVD7gujY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.russellbryant.net/&quot;&gt;Russell Bryant&lt;/a&gt; Russell Bryant is a Distinguished Engineer at Red Hat, where he works on infrastructure management to support Kubernetes clusters. Prior to working on the Metal³ project, Russell has worked on other open infrastructure projects. Russell worked in Software Defined Networking with Open vSwitch (OVS) and Open Virtual Network (OVN) and worked on various parts of OpenStack. Russell also worked in open source telephony via the Asterisk project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://twitter.com/doughellmann&quot;&gt;Doug Hellmann&lt;/a&gt; Doug Hellmann is a Senior Principal Software Engineer at Red Hat. He has been a professional developer since the mid 1990s and has worked on a variety of projects in fields such as mapping, medical news publishing, banking, data center automation, and hardware provisioning. He has been contributing to open source projects for most of his career and for the past 7 years he has been focusing on open source cloud computing technologies, including OpenStack and Kubernetes.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://static.sched.com/hosted_files/kccncna19/b3/Introducing%20Metal3%20KubeCon%20NA%202019.pdf&quot;&gt;Presentation: Introducing Metal³ KubeCon NA 2019 PDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KIIkVD7gujY&amp;amp;feature=emb_logo&quot;&gt;Video: Introducing Metal³: Kubernetes Native Bare Metal Host Management video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;demos&quot;&gt;Demos&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://asciinema.org/a/uOCLoCiOlMLMBLuHOcV2ZvZxb&quot;&gt;First demo (Inspection)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/283704&quot;&gt;&lt;img src=&quot;https://asciinema.org/a/283704.svg&quot; alt=&quot;asciicast&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://asciinema.org/a/Z4a4MhXd7DStprfyiiworS2Id&quot;&gt;Second demo (Provisioning)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/283705&quot;&gt;&lt;img src=&quot;https://asciinema.org/a/283705.svg&quot; alt=&quot;asciicast&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://asciinema.org/a/Xs5BPe62kF1PyIkNvMkcC9lyt&quot;&gt;Third demo (Scale up)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/283706&quot;&gt;&lt;img src=&quot;https://asciinema.org/a/283706.svg&quot; alt=&quot;asciicast&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://asciinema.org/a/c5BUvn2iK1J076dI3xLNe4H9C&quot;&gt;Fourth demo (v1alpha2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/283707&quot;&gt;&lt;img src=&quot;https://asciinema.org/a/283707.svg&quot; alt=&quot;asciicast&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Pedro Ibáñez Requena</name></author><summary type="html">Conference talk: Introducing Metal³: Kubernetes Native Bare Metal Host Management - Russell Bryant &amp;amp; Doug Hellmann, Red Hat</summary></entry><entry><title type="html">Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019</title><link href="https://metal3.io/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html" rel="alternate" type="text/html" title="Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019" /><published>2019-11-13T08:37:00+00:00</published><updated>2019-11-13T08:37:00+00:00</updated><id>https://metal3.io/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit</id><content type="html" xml:base="https://metal3.io/blog/2019/11/13/Extend_Your_Data_Center_to_the_Hybrid_Edge-Red_Hat_Summit.html">&lt;h2 id=&quot;conference-talk-extend-your-data-center-to-the-hybrid-edge---red-hat-summit-may-2019-paul-cormier-burr-stutter-and-garima-sharma&quot;&gt;Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma&lt;/h2&gt;

&lt;p&gt;A critical part of being successful in the hybrid cloud is being successful in your data center with your own infrastructure.&lt;/p&gt;

&lt;p&gt;In this video, Paul Cormier, Burr Sutter and Garima Sharma show how you can bring the Open Hybrid cloud to the edge. Cluster management from multiple cloud providers to on premise. In the demo you’ll see a multi-cluster inventory for the open hybrid cloud at cloud.redhat.com, OpenShift Container Storage providing storage for Virtual Machines and containers (Cloud, Virtualization and bare metal), and everything Kubernetes native.&lt;/p&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.redhat.com/en/about/company/management/paul-cormier&quot;&gt;Paul Cormier&lt;/a&gt; Executive vice president and president, Products and Technologies. Leads Red Hat’s technology and products organizations, including engineering, product management, and product marketing for Red Hat’s technologies. He joined Red Hat in May 2001 as executive vice president, Engineering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://burrsutter.com/&quot;&gt;Burr Sutter&lt;/a&gt; A lifelong developer advocate, community organizer, and technology evangelist, Burr Sutter is a featured speaker at technology events around the globe —from Bangalore to Brussels and Berlin to Beijing (and most parts in between)— he is currently Director of Developer Experience at Red Hat. A Java Champion since 2005 and former president of the Atlanta Java User Group, Burr founded the DevNexus conference —now the second largest Java event in the U.S.— with the aim of making access to the world’s leading developers affordable to the developer community.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/garimasharma/&quot;&gt;Garima Sharma&lt;/a&gt; Senior Engineering leader at world’s largest Open Source company. As a seasoned Tech professional, she runs a global team of Solutions Engineers focused on a large portfolio of Cloud Computing products and technology. She has helped shape science and technology for mission critical software, reliability in operations and re-design of architecture all geared towards advancements in medicine, security, cloud technologies and bottom line savings for the client businesses. Whether leading the architecture, development and delivery of customer centric cutting edge systems, or spearheading diversity and inclusion initiatives via keynotes, blogs and conference presentations, Garima champions the idea of STEM. Garima ardently believes in Maya Angelou’s message that diversity makes for a rich tapestry, and we must understand that all the threads of the tapestry are equal in value no matter what their color.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.pscp.tv/RedHatOfficial/1vAGRWYPjngJl?t=1h27m51s&quot;&gt;Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><summary type="html">Conference talk: Extend Your Data Center to the Hybrid Edge - Red Hat Summit, May 2019, Paul Cormier, Burr Stutter and Garima Sharma</summary></entry><entry><title type="html">Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat</title><link href="https://metal3.io/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html" rel="alternate" type="text/html" title="Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat" /><published>2019-11-07T11:02:00+00:00</published><updated>2019-11-07T11:02:00+00:00</updated><id>https://metal3.io/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic</id><content type="html" xml:base="https://metal3.io/blog/2019/11/07/Kubernetes-native_Infrastructure-Managed_Baremetal_with_Kubernetes_Operators_and_OpenStack_Ironic.html">&lt;h2 id=&quot;conference-talk-open-infrastructure-days-uk-2019-kubernetes-native-infrastructure-managed-baremetal-with-kubernetes-operators-and-openstack-ironic---steve-hardy-red-hat&quot;&gt;Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat&lt;/h2&gt;

&lt;p&gt;In this session you can hear about a new effort to enable baremetal Kubernetes deployments using native interfaces, and in particular the Kubernetes Operator framework, combined with OpenStack Ironic.&lt;/p&gt;

&lt;p&gt;This approach aims to seamlessly integrate your infrastructure with your workloads, including baremetal servers, storage and container/VM workloads. All this can be achieved using kubernetes native applications, combined with existing, proven deployment and storage tooling.&lt;/p&gt;

&lt;p&gt;In this talk we cover the options around Kubernetes deployments today, the specific approach taken by the new Kubernetes-native “MetalKube” project, and the status/roadmap of this new community effort.&lt;/p&gt;

&lt;h2 id=&quot;speakers&quot;&gt;Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://hardysteven.blogspot.com&quot;&gt;Steve Hardy&lt;/a&gt; is Senior Principal Software Engineer at Red Hat, currently involved in kubernetes/OpenShift deployment and architecture. He is also an active member of the OpenStack community, and has been a project team lead of both the Heat (orchestration) and TripleO (deployment) projects.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openinfradays.sched.com/event/KMyE&quot;&gt;Open Infrastructure Days UK 2019, Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pedro Ibáñez Requena</name></author><summary type="html">Conference talk: Open Infrastructure Days UK 2019; Kubernetes-native Infrastructure: Managed Baremetal with Kubernetes Operators and OpenStack Ironic - Steve Hardy, Red Hat</summary></entry></feed>